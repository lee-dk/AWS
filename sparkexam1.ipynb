{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **pyspark 패키지를 활용한 Spark 프로그래밍**\n",
    "## SparkSession 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://121.133.223.69:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sparkedu</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x192a88cf3a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[2]\") \\\n",
    "                    .appName('sparkedu') \\\n",
    "                    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark1](images/spark1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리스트객체로 RDD 객체 생성하기\n",
    "\n",
    "### RDD(Resilient Distributed Dataset)\n",
    "#### read-only 데이터셋으로서 다양한 머신에 데이터셋의 멀티셋(중복을 허용)을 분산해두고 특정한 머신에 문제가 생기더라도 문제없이 읽을수로 있도록 지원한다\n",
    "\n",
    "- MapReduce 작업\n",
    "- 분산하여 병렬적 처리\n",
    "- 빠른 연산\n",
    "- 불변(Immutable)\n",
    "- Transformation 과 Action 으로 함수 종류가 나눠지며, Action 함수가 실행됐을 때 실제 연산\n",
    "- Lineage 를 통해 Fault Tolerant(내고장성) 보장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark2](images/spark2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "[('Java', 20000), ('Python', 100000), ('Scala', 3000)]\n"
     ]
    }
   ],
   "source": [
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "rdd=spark.sparkContext.parallelize(dataList)\n",
    "print(rdd)\n",
    "print(type(rdd))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "[2, 9, 7, 1, 9, 8, 7, 4, 3, 7, 9, 1, 0, 3, 9, 1, 2, 0, 2, 5]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lst=np.random.randint(0,10,20)\n",
    "rdd=spark.sparkContext.parallelize(lst)\n",
    "print(type(rdd))\n",
    "print(rdd.collect())\n",
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 파일 내용 읽어서 RDD 객체 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(value='아'), Row(value='휴'), Row(value='아이구'), Row(value='아이쿠'), Row(value='아이고'), Row(value='어'), Row(value='나'), Row(value='우리'), Row(value='저희'), Row(value='따라'), Row(value='의해'), Row(value='을'), Row(value='를'), Row(value='에'), Row(value='의'), Row(value='가'), Row(value='으로'), Row(value='로'), Row(value='에게'), Row(value='뿐이다'), Row(value='의거하여'), Row(value='근거하여'), Row(value='입각하여'), Row(value='기준으로'), Row(value='예하면'), Row(value='예를 들면'), Row(value='예를 들자면'), Row(value='저'), Row(value='소인'), Row(value='소생'), Row(value='저희'), Row(value='지말고'), Row(value='하지마'), Row(value='하지마라'), Row(value='다른'), Row(value='물론'), Row(value='또한'), Row(value='그리고'), Row(value='비길수 없다'), Row(value='해서는 안된다'), Row(value='뿐만 아니라'), Row(value='만이 아니다'), Row(value='만은 아니다'), Row(value='막론하고'), Row(value='관계없이'), Row(value='그치지 않다'), Row(value='그러나'), Row(value='그런데'), Row(value='하지만'), Row(value='든간에'), Row(value='논하지 않다'), Row(value='따지지 않다'), Row(value='설사'), Row(value='비록'), Row(value='더라도'), Row(value='아니면'), Row(value='만 못하다'), Row(value='하는 편이 낫다'), Row(value='불문하고'), Row(value='향하여'), Row(value='향해서'), Row(value='향하다'), Row(value='쪽으로'), Row(value='틈타'), Row(value='이용하여'), Row(value='타다'), Row(value='오르다'), Row(value='제외하고'), Row(value='이 외에'), Row(value='이 밖에'), Row(value='하여야'), Row(value='비로소'), Row(value='한다면 몰라도'), Row(value='외에도'), Row(value='이곳'), Row(value='여기'), Row(value='부터'), Row(value='기점으로'), Row(value='따라서'), Row(value='할 생각이다'), Row(value='하려고하다'), Row(value='이리하여'), Row(value='그리하여'), Row(value='그렇게 함으로써'), Row(value='하지만'), Row(value='일때'), Row(value='할때'), Row(value='앞에서'), Row(value='중에서'), Row(value='보는데서'), Row(value='으로써'), Row(value='로써'), Row(value='까지'), Row(value='해야한다'), Row(value='일것이다'), Row(value='반드시'), Row(value='할줄알다'), Row(value='할수있다'), Row(value='할수있어'), Row(value='임에 틀림없다'), Row(value='한다면'), Row(value='등'), Row(value='등등'), Row(value='제'), Row(value='겨우'), Row(value='단지'), Row(value='다만'), Row(value='할뿐'), Row(value='딩동'), Row(value='댕그'), Row(value='대해서'), Row(value='대하여'), Row(value='대하면'), Row(value='훨씬'), Row(value='얼마나'), Row(value='얼마만큼'), Row(value='얼마큼'), Row(value='남짓'), Row(value='여'), Row(value='얼마간'), Row(value='약간'), Row(value='다소'), Row(value='좀'), Row(value='조금'), Row(value='다수'), Row(value='몇'), Row(value='얼마'), Row(value='지만'), Row(value='하물며'), Row(value='또한'), Row(value='그러나'), Row(value='그렇지만'), Row(value='하지만'), Row(value='이외에도'), Row(value='대해 말하자면'), Row(value='뿐이다'), Row(value='다음에'), Row(value='반대로'), Row(value='반대로 말하자면'), Row(value='이와 반대로'), Row(value='바꾸어서 말하면'), Row(value='바꾸어서 한다면'), Row(value='만약'), Row(value='그렇지않으면'), Row(value='까악'), Row(value='툭'), Row(value='딱'), Row(value='삐걱거리다'), Row(value='보드득'), Row(value='비걱거리다'), Row(value='꽈당'), Row(value='응당'), Row(value='해야한다'), Row(value='에 가서'), Row(value='각'), Row(value='각각'), Row(value='여러분'), Row(value='각종'), Row(value='각자'), Row(value='제각기'), Row(value='하도록하다'), Row(value='와'), Row(value='과'), Row(value='그러므로'), Row(value='그래서'), Row(value='고로'), Row(value='한 까닭에'), Row(value='하기 때문에'), Row(value='거니와'), Row(value='이지만'), Row(value='대하여'), Row(value='관하여'), Row(value='관한'), Row(value='과연'), Row(value='실로'), Row(value='아니나다를가'), Row(value='생각한대로'), Row(value='진짜로'), Row(value='한적이있다'), Row(value='하곤하였다'), Row(value='하'), Row(value='하하'), Row(value='허허'), Row(value='아하'), Row(value='거바'), Row(value='와'), Row(value='오'), Row(value='왜'), Row(value='어째서'), Row(value='무엇때문에'), Row(value='어찌'), Row(value='하겠는가'), Row(value='무슨'), Row(value='어디'), Row(value='어느곳'), Row(value='더군다나'), Row(value='하물며'), Row(value='더욱이는'), Row(value='어느때'), Row(value='언제'), Row(value='야'), Row(value='이봐'), Row(value='어이'), Row(value='여보시오'), Row(value='흐흐'), Row(value='흥'), Row(value='휴'), Row(value='헉헉'), Row(value='헐떡헐떡'), Row(value='영차'), Row(value='여차'), Row(value='어기여차'), Row(value='끙끙'), Row(value='아야'), Row(value='앗'), Row(value='아야'), Row(value='콸콸'), Row(value='졸졸'), Row(value='좍좍'), Row(value='뚝뚝'), Row(value='주룩주룩'), Row(value='솨'), Row(value='우르르'), Row(value='그래도'), Row(value='또'), Row(value='그리고'), Row(value='바꾸어말하면'), Row(value='바꾸어말하자면'), Row(value='혹은'), Row(value='혹시'), Row(value='답다'), Row(value='및'), Row(value='그에 따르는'), Row(value='때가 되어'), Row(value='즉'), Row(value='지든지'), Row(value='설령'), Row(value='가령'), Row(value='하더라도'), Row(value='할지라도'), Row(value='일지라도'), Row(value='지든지'), Row(value='몇'), Row(value='거의'), Row(value='하마터면'), Row(value='인젠'), Row(value='이젠'), Row(value='된바에야'), Row(value='된이상'), Row(value='만큼\\t어찌됏든'), Row(value='그위에'), Row(value='게다가'), Row(value='점에서 보아'), Row(value='비추어 보아'), Row(value='고려하면'), Row(value='하게될것이다'), Row(value='일것이다'), Row(value='비교적'), Row(value='좀'), Row(value='보다더'), Row(value='비하면'), Row(value='시키다'), Row(value='하게하다'), Row(value='할만하다'), Row(value='의해서'), Row(value='연이서'), Row(value='이어서'), Row(value='잇따라'), Row(value='뒤따라'), Row(value='뒤이어'), Row(value='결국'), Row(value='의지하여'), Row(value='기대여'), Row(value='통하여'), Row(value='자마자'), Row(value='더욱더'), Row(value='불구하고'), Row(value='얼마든지'), Row(value='마음대로'), Row(value='주저하지 않고'), Row(value='곧'), Row(value='즉시'), Row(value='바로'), Row(value='당장'), Row(value='하자마자'), Row(value='밖에 안된다'), Row(value='하면된다'), Row(value='그래'), Row(value='그렇지'), Row(value='요컨대'), Row(value='다시 말하자면'), Row(value='바꿔 말하면'), Row(value='즉'), Row(value='구체적으로'), Row(value='말하자면'), Row(value='시작하여'), Row(value='시초에'), Row(value='이상'), Row(value='허'), Row(value='헉'), Row(value='허걱'), Row(value='바와같이'), Row(value='해도좋다'), Row(value='해도된다'), Row(value='게다가'), Row(value='더구나'), Row(value='하물며'), Row(value='와르르'), Row(value='팍'), Row(value='퍽'), Row(value='펄렁'), Row(value='동안'), Row(value='이래'), Row(value='하고있었다'), Row(value='이었다'), Row(value='에서'), Row(value='로부터'), Row(value='까지'), Row(value='예하면'), Row(value='했어요'), Row(value='해요'), Row(value='함께'), Row(value='같이'), Row(value='더불어'), Row(value='마저'), Row(value='마저도'), Row(value='양자'), Row(value='모두'), Row(value='습니다'), Row(value='가까스로'), Row(value='하려고하다'), Row(value='즈음하여'), Row(value='다른'), Row(value='다른 방면으로'), Row(value='해봐요'), Row(value='습니까'), Row(value='했어요'), Row(value='말할것도 없고'), Row(value='무릎쓰고'), Row(value='개의치않고'), Row(value='하는것만 못하다'), Row(value='하는것이 낫다'), Row(value='매'), Row(value='매번'), Row(value='들'), Row(value='모'), Row(value='어느것'), Row(value='어느'), Row(value='로써'), Row(value='갖고말하자면'), Row(value='어디'), Row(value='어느쪽'), Row(value='어느것'), Row(value='어느해'), Row(value='어느 년도'), Row(value='라 해도'), Row(value='언젠가'), Row(value='어떤것'), Row(value='어느것'), Row(value='저기'), Row(value='저쪽'), Row(value='저것'), Row(value='그때'), Row(value='그럼'), Row(value='그러면'), Row(value='요만한걸'), Row(value='그래'), Row(value='그때'), Row(value='저것만큼'), Row(value='그저'), Row(value='이르기까지'), Row(value='할 줄 안다'), Row(value='할 힘이 있다'), Row(value='너'), Row(value='너희'), Row(value='당신'), Row(value='어찌'), Row(value='설마'), Row(value='차라리'), Row(value='할지언정'), Row(value='할지라도'), Row(value='할망정'), Row(value='할지언정'), Row(value='구토하다'), Row(value='게우다'), Row(value='토하다'), Row(value='메쓰겁다'), Row(value='옆사람'), Row(value='퉤'), Row(value='쳇'), Row(value='의거하여'), Row(value='근거하여'), Row(value='의해'), Row(value='따라'), Row(value='힘입어'), Row(value='그'), Row(value='다음'), Row(value='버금'), Row(value='두번째로'), Row(value='기타'), Row(value='첫번째로'), Row(value='나머지는'), Row(value='그중에서'), Row(value='견지에서'), Row(value='형식으로 쓰여'), Row(value='입장에서'), Row(value='위해서'), Row(value='단지'), Row(value='의해되다'), Row(value='하도록시키다'), Row(value='뿐만아니라'), Row(value='반대로'), Row(value='전후'), Row(value='전자'), Row(value='앞의것'), Row(value='잠시'), Row(value='잠깐'), Row(value='하면서'), Row(value='그렇지만'), Row(value='다음에'), Row(value='그러한즉'), Row(value='그런즉'), Row(value='남들'), Row(value='아무거나'), Row(value='어찌하든지'), Row(value='같다'), Row(value='비슷하다'), Row(value='예컨대'), Row(value='이럴정도로'), Row(value='어떻게'), Row(value='만약'), Row(value='만일'), Row(value='위에서 서술한바와같이'), Row(value='인 듯하다'), Row(value='하지 않는다면'), Row(value='만약에'), Row(value='무엇'), Row(value='무슨'), Row(value='어느'), Row(value='어떤'), Row(value='아래윗'), Row(value='조차'), Row(value='한데'), Row(value='그럼에도 불구하고'), Row(value='여전히'), Row(value='심지어'), Row(value='까지도'), Row(value='조차도'), Row(value='하지 않도록'), Row(value='않기 위하여'), Row(value='때'), Row(value='시각'), Row(value='무렵'), Row(value='시간'), Row(value='동안'), Row(value='어때'), Row(value='어떠한'), Row(value='하여금'), Row(value='네'), Row(value='예'), Row(value='우선'), Row(value='누구'), Row(value='누가 알겠는가'), Row(value='아무도'), Row(value='줄은모른다'), Row(value='줄은 몰랏다'), Row(value='하는 김에'), Row(value='겸사겸사'), Row(value='하는바'), Row(value='그런 까닭에'), Row(value='한 이유는'), Row(value='그러니'), Row(value='그러니까'), Row(value='때문에'), Row(value='그'), Row(value='너희'), Row(value='그들'), Row(value='너희들'), Row(value='타인'), Row(value='것'), Row(value='것들'), Row(value='너'), Row(value='위하여'), Row(value='공동으로'), Row(value='동시에'), Row(value='하기 위하여'), Row(value='어찌하여'), Row(value='무엇때문에'), Row(value='붕붕'), Row(value='윙윙'), Row(value='나'), Row(value='우리'), Row(value='엉엉'), Row(value='휘익'), Row(value='윙윙'), Row(value='오호'), Row(value='아하'), Row(value='어쨋든'), Row(value='만 못하다\\t하기보다는'), Row(value='차라리'), Row(value='하는 편이 낫다'), Row(value='흐흐'), Row(value='놀라다'), Row(value='상대적으로 말하자면'), Row(value='마치'), Row(value='아니라면'), Row(value='쉿'), Row(value='그렇지 않으면'), Row(value='그렇지 않다면'), Row(value='안 그러면'), Row(value='아니었다면'), Row(value='하든지'), Row(value='아니면'), Row(value='이라면'), Row(value='좋아'), Row(value='알았어'), Row(value='하는것도'), Row(value='그만이다'), Row(value='어쩔수 없다'), Row(value='하나'), Row(value='일'), Row(value='일반적으로'), Row(value='일단'), Row(value='한켠으로는'), Row(value='오자마자'), Row(value='이렇게되면'), Row(value='이와같다면'), Row(value='전부'), Row(value='한마디'), Row(value='한항목'), Row(value='근거로'), Row(value='하기에'), Row(value='아울러'), Row(value='하지 않도록'), Row(value='않기 위해서'), Row(value='이르기까지'), Row(value='이 되다'), Row(value='로 인하여'), Row(value='까닭으로'), Row(value='이유만으로'), Row(value='이로 인하여'), Row(value='그래서'), Row(value='이 때문에'), Row(value='그러므로'), Row(value='그런 까닭에'), Row(value='알 수 있다'), Row(value='결론을 낼 수 있다'), Row(value='으로 인하여'), Row(value='있다'), Row(value='어떤것'), Row(value='관계가 있다'), Row(value='관련이 있다'), Row(value='연관되다'), Row(value='어떤것들'), Row(value='에 대해'), Row(value='이리하여'), Row(value='그리하여'), Row(value='여부'), Row(value='하기보다는'), Row(value='하느니'), Row(value='하면 할수록'), Row(value='운운'), Row(value='이러이러하다'), Row(value='하구나'), Row(value='하도다'), Row(value='다시말하면'), Row(value='다음으로'), Row(value='에 있다'), Row(value='에 달려 있다'), Row(value='우리'), Row(value='우리들'), Row(value='오히려'), Row(value='하기는한데'), Row(value='어떻게'), Row(value='어떻해'), Row(value='어찌됏어'), Row(value='어때'), Row(value='어째서'), Row(value='본대로'), Row(value='자'), Row(value='이'), Row(value='이쪽'), Row(value='여기'), Row(value='이것'), Row(value='이번'), Row(value='이렇게말하자면'), Row(value='이런'), Row(value='이러한'), Row(value='이와 같은'), Row(value='요만큼'), Row(value='요만한 것'), Row(value='얼마 안 되는 것'), Row(value='이만큼'), Row(value='이 정도의'), Row(value='이렇게 많은 것'), Row(value='이와 같다'), Row(value='이때'), Row(value='이렇구나'), Row(value='것과 같이'), Row(value='끼익'), Row(value='삐걱'), Row(value='따위'), Row(value='와 같은 사람들'), Row(value='부류의 사람들'), Row(value='왜냐하면'), Row(value='중의하나'), Row(value='오직'), Row(value='오로지'), Row(value='에 한하다'), Row(value='하기만 하면'), Row(value='도착하다'), Row(value='까지 미치다'), Row(value='도달하다'), Row(value='정도에 이르다'), Row(value='할 지경이다'), Row(value='결과에 이르다'), Row(value='관해서는'), Row(value='여러분'), Row(value='하고 있다'), Row(value='한 후'), Row(value='혼자'), Row(value='자기'), Row(value='자기집'), Row(value='자신'), Row(value='우에 종합한것과같이'), Row(value='총적으로 보면'), Row(value='총적으로 말하면'), Row(value='총적으로'), Row(value='대로 하다'), Row(value='으로서'), Row(value='참'), Row(value='그만이다'), Row(value='할 따름이다'), Row(value='쿵'), Row(value='탕탕'), Row(value='쾅쾅'), Row(value='둥둥'), Row(value='봐'), Row(value='봐라'), Row(value='아이야'), Row(value='아니'), Row(value='와아'), Row(value='응'), Row(value='아이'), Row(value='참나'), Row(value='년'), Row(value='월'), Row(value='일'), Row(value='령'), Row(value='영'), Row(value='일'), Row(value='이'), Row(value='삼'), Row(value='사'), Row(value='오'), Row(value='육'), Row(value='륙'), Row(value='칠'), Row(value='팔'), Row(value='구'), Row(value='이천육'), Row(value='이천칠'), Row(value='이천팔'), Row(value='이천구'), Row(value='하나'), Row(value='둘'), Row(value='셋'), Row(value='넷'), Row(value='다섯'), Row(value='여섯'), Row(value='일곱'), Row(value='여덟'), Row(value='아홉'), Row(value='령'), Row(value='영')]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.read.text(\"data/korean_stopwords.txt\")\n",
    "print(type(rdd))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "['아', '휴', '아이구', '아이쿠', '아이고', '어', '나', '우리', '저희', '따라', '의해', '을', '를', '에', '의', '가', '으로', '로', '에게', '뿐이다', '의거하여', '근거하여', '입각하여', '기준으로', '예하면', '예를 들면', '예를 들자면', '저', '소인', '소생', '저희', '지말고', '하지마', '하지마라', '다른', '물론', '또한', '그리고', '비길수 없다', '해서는 안된다', '뿐만 아니라', '만이 아니다', '만은 아니다', '막론하고', '관계없이', '그치지 않다', '그러나', '그런데', '하지만', '든간에', '논하지 않다', '따지지 않다', '설사', '비록', '더라도', '아니면', '만 못하다', '하는 편이 낫다', '불문하고', '향하여', '향해서', '향하다', '쪽으로', '틈타', '이용하여', '타다', '오르다', '제외하고', '이 외에', '이 밖에', '하여야', '비로소', '한다면 몰라도', '외에도', '이곳', '여기', '부터', '기점으로', '따라서', '할 생각이다', '하려고하다', '이리하여', '그리하여', '그렇게 함으로써', '하지만', '일때', '할때', '앞에서', '중에서', '보는데서', '으로써', '로써', '까지', '해야한다', '일것이다', '반드시', '할줄알다', '할수있다', '할수있어', '임에 틀림없다', '한다면', '등', '등등', '제', '겨우', '단지', '다만', '할뿐', '딩동', '댕그', '대해서', '대하여', '대하면', '훨씬', '얼마나', '얼마만큼', '얼마큼', '남짓', '여', '얼마간', '약간', '다소', '좀', '조금', '다수', '몇', '얼마', '지만', '하물며', '또한', '그러나', '그렇지만', '하지만', '이외에도', '대해 말하자면', '뿐이다', '다음에', '반대로', '반대로 말하자면', '이와 반대로', '바꾸어서 말하면', '바꾸어서 한다면', '만약', '그렇지않으면', '까악', '툭', '딱', '삐걱거리다', '보드득', '비걱거리다', '꽈당', '응당', '해야한다', '에 가서', '각', '각각', '여러분', '각종', '각자', '제각기', '하도록하다', '와', '과', '그러므로', '그래서', '고로', '한 까닭에', '하기 때문에', '거니와', '이지만', '대하여', '관하여', '관한', '과연', '실로', '아니나다를가', '생각한대로', '진짜로', '한적이있다', '하곤하였다', '하', '하하', '허허', '아하', '거바', '와', '오', '왜', '어째서', '무엇때문에', '어찌', '하겠는가', '무슨', '어디', '어느곳', '더군다나', '하물며', '더욱이는', '어느때', '언제', '야', '이봐', '어이', '여보시오', '흐흐', '흥', '휴', '헉헉', '헐떡헐떡', '영차', '여차', '어기여차', '끙끙', '아야', '앗', '아야', '콸콸', '졸졸', '좍좍', '뚝뚝', '주룩주룩', '솨', '우르르', '그래도', '또', '그리고', '바꾸어말하면', '바꾸어말하자면', '혹은', '혹시', '답다', '및', '그에 따르는', '때가 되어', '즉', '지든지', '설령', '가령', '하더라도', '할지라도', '일지라도', '지든지', '몇', '거의', '하마터면', '인젠', '이젠', '된바에야', '된이상', '만큼\\t어찌됏든', '그위에', '게다가', '점에서 보아', '비추어 보아', '고려하면', '하게될것이다', '일것이다', '비교적', '좀', '보다더', '비하면', '시키다', '하게하다', '할만하다', '의해서', '연이서', '이어서', '잇따라', '뒤따라', '뒤이어', '결국', '의지하여', '기대여', '통하여', '자마자', '더욱더', '불구하고', '얼마든지', '마음대로', '주저하지 않고', '곧', '즉시', '바로', '당장', '하자마자', '밖에 안된다', '하면된다', '그래', '그렇지', '요컨대', '다시 말하자면', '바꿔 말하면', '즉', '구체적으로', '말하자면', '시작하여', '시초에', '이상', '허', '헉', '허걱', '바와같이', '해도좋다', '해도된다', '게다가', '더구나', '하물며', '와르르', '팍', '퍽', '펄렁', '동안', '이래', '하고있었다', '이었다', '에서', '로부터', '까지', '예하면', '했어요', '해요', '함께', '같이', '더불어', '마저', '마저도', '양자', '모두', '습니다', '가까스로', '하려고하다', '즈음하여', '다른', '다른 방면으로', '해봐요', '습니까', '했어요', '말할것도 없고', '무릎쓰고', '개의치않고', '하는것만 못하다', '하는것이 낫다', '매', '매번', '들', '모', '어느것', '어느', '로써', '갖고말하자면', '어디', '어느쪽', '어느것', '어느해', '어느 년도', '라 해도', '언젠가', '어떤것', '어느것', '저기', '저쪽', '저것', '그때', '그럼', '그러면', '요만한걸', '그래', '그때', '저것만큼', '그저', '이르기까지', '할 줄 안다', '할 힘이 있다', '너', '너희', '당신', '어찌', '설마', '차라리', '할지언정', '할지라도', '할망정', '할지언정', '구토하다', '게우다', '토하다', '메쓰겁다', '옆사람', '퉤', '쳇', '의거하여', '근거하여', '의해', '따라', '힘입어', '그', '다음', '버금', '두번째로', '기타', '첫번째로', '나머지는', '그중에서', '견지에서', '형식으로 쓰여', '입장에서', '위해서', '단지', '의해되다', '하도록시키다', '뿐만아니라', '반대로', '전후', '전자', '앞의것', '잠시', '잠깐', '하면서', '그렇지만', '다음에', '그러한즉', '그런즉', '남들', '아무거나', '어찌하든지', '같다', '비슷하다', '예컨대', '이럴정도로', '어떻게', '만약', '만일', '위에서 서술한바와같이', '인 듯하다', '하지 않는다면', '만약에', '무엇', '무슨', '어느', '어떤', '아래윗', '조차', '한데', '그럼에도 불구하고', '여전히', '심지어', '까지도', '조차도', '하지 않도록', '않기 위하여', '때', '시각', '무렵', '시간', '동안', '어때', '어떠한', '하여금', '네', '예', '우선', '누구', '누가 알겠는가', '아무도', '줄은모른다', '줄은 몰랏다', '하는 김에', '겸사겸사', '하는바', '그런 까닭에', '한 이유는', '그러니', '그러니까', '때문에', '그', '너희', '그들', '너희들', '타인', '것', '것들', '너', '위하여', '공동으로', '동시에', '하기 위하여', '어찌하여', '무엇때문에', '붕붕', '윙윙', '나', '우리', '엉엉', '휘익', '윙윙', '오호', '아하', '어쨋든', '만 못하다\\t하기보다는', '차라리', '하는 편이 낫다', '흐흐', '놀라다', '상대적으로 말하자면', '마치', '아니라면', '쉿', '그렇지 않으면', '그렇지 않다면', '안 그러면', '아니었다면', '하든지', '아니면', '이라면', '좋아', '알았어', '하는것도', '그만이다', '어쩔수 없다', '하나', '일', '일반적으로', '일단', '한켠으로는', '오자마자', '이렇게되면', '이와같다면', '전부', '한마디', '한항목', '근거로', '하기에', '아울러', '하지 않도록', '않기 위해서', '이르기까지', '이 되다', '로 인하여', '까닭으로', '이유만으로', '이로 인하여', '그래서', '이 때문에', '그러므로', '그런 까닭에', '알 수 있다', '결론을 낼 수 있다', '으로 인하여', '있다', '어떤것', '관계가 있다', '관련이 있다', '연관되다', '어떤것들', '에 대해', '이리하여', '그리하여', '여부', '하기보다는', '하느니', '하면 할수록', '운운', '이러이러하다', '하구나', '하도다', '다시말하면', '다음으로', '에 있다', '에 달려 있다', '우리', '우리들', '오히려', '하기는한데', '어떻게', '어떻해', '어찌됏어', '어때', '어째서', '본대로', '자', '이', '이쪽', '여기', '이것', '이번', '이렇게말하자면', '이런', '이러한', '이와 같은', '요만큼', '요만한 것', '얼마 안 되는 것', '이만큼', '이 정도의', '이렇게 많은 것', '이와 같다', '이때', '이렇구나', '것과 같이', '끼익', '삐걱', '따위', '와 같은 사람들', '부류의 사람들', '왜냐하면', '중의하나', '오직', '오로지', '에 한하다', '하기만 하면', '도착하다', '까지 미치다', '도달하다', '정도에 이르다', '할 지경이다', '결과에 이르다', '관해서는', '여러분', '하고 있다', '한 후', '혼자', '자기', '자기집', '자신', '우에 종합한것과같이', '총적으로 보면', '총적으로 말하면', '총적으로', '대로 하다', '으로서', '참', '그만이다', '할 따름이다', '쿵', '탕탕', '쾅쾅', '둥둥', '봐', '봐라', '아이야', '아니', '와아', '응', '아이', '참나', '년', '월', '일', '령', '영', '일', '이', '삼', '사', '오', '육', '륙', '칠', '팔', '구', '이천육', '이천칠', '이천팔', '이천구', '하나', '둘', '셋', '넷', '다섯', '여섯', '일곱', '여덟', '아홉', '령', '영']\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile(\"data/korean_stopwords.txt\")\n",
    "print(type(rdd))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 생성한 RDD 객체 Spark DataFrame 으로 변환하기\n",
    "\n",
    "### Spark DataFrame\n",
    "- DataFrame은 명명 된 열로 구성된 데이터 세트 \n",
    "- 개념적으로는 관계형 데이터베이스의 테이블 또는 R / Python의 데이터 프레임과 동일하지만 내부적으로 더욱  최적화가 있음\n",
    "- RDB Table처럼 Schema를 가지고 있고 RDB의 Table 연산이 가능\n",
    "- 구조화 된 데이터 파일, Hive의 테이블, 외부 데이터베이스 또는 기존 RDD와 같은 다양한 소스 에서 구성 할 수 있늠 \n",
    "- DataFrame API는 Scala, Java, Python 및 R 에서 사용할 수 있음\n",
    "- SparkSQL을 통해 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Finance', 10), ('Marketing', 20), ('Sales', 30), ('IT', 40)]\n"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\",10), \n",
    "        (\"Marketing\",20), \n",
    "        (\"Sales\",30), \n",
    "        (\"IT\",40) \n",
    "      ]\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---------+---+\n",
      "|       _1| _2|\n",
      "+---------+---+\n",
      "|  Finance| 10|\n",
      "|Marketing| 20|\n",
      "|    Sales| 30|\n",
      "|       IT| 40|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "df2 = rdd.toDF(deptColumns)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "DataFrame[firstname: string, middlename: string, lastname: string, dob: string, gender: string, salary: bigint]\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "print(type(df))\n",
    "print(df)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data2,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV 파일 내용 읽어서 DataFrame 객체 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      "\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|  _c0|   _c1|      _c2| _c3|       _c4| _c5| _c6|   _c7|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08|1500|null|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25|1300|null|    10|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/emp.csv\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: string (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: string (nullable = true)\n",
      " |-- hiredate: string (nullable = true)\n",
      " |-- sal: string (nullable = true)\n",
      " |-- comm: string (nullable = true)\n",
      " |-- deptno: string (nullable = true)\n",
      "\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08|1500|null|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25|1300|null|    10|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = spark.read.csv(\"data/emp.csv\", header=True)\n",
    "emp.printSchema()\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: integer (nullable = true)\n",
      " |-- hiredate: string (nullable = true)\n",
      " |-- sal: integer (nullable = true)\n",
      " |-- comm: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      "\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08|1500|null|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25|1300|null|    10|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = spark.read.csv(\"data/emp.csv\", header=True, inferSchema=True)\n",
    "emp.printSchema()\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      "\n",
      "+---+---------+------------+----------+------+------------+----------+\n",
      "|_c0|      _c1|         _c2|       _c3|   _c4|         _c5|       _c6|\n",
      "+---+---------+------------+----------+------+------------+----------+\n",
      "|mpg|cylinders|displacement|horsepower|weight|acceleration|model-year|\n",
      "| 18|        8|         307|       130|  3504|          12|        70|\n",
      "| 15|        8|         350|       165|  3693|        11.5|        70|\n",
      "| 18|        8|         318|       150|  3436|          11|        70|\n",
      "| 16|        8|         304|       150|  3433|          12|        70|\n",
      "| 17|        8|         302|       140|  3449|        10.5|        70|\n",
      "| 15|        8|         429|       198|  4341|          10|        70|\n",
      "| 14|        8|         454|       220|  4354|           9|        70|\n",
      "| 14|        8|         440|       215|  4312|         8.5|        70|\n",
      "| 14|        8|         455|       225|  4425|          10|        70|\n",
      "| 15|        8|         390|       190|  3850|         8.5|        70|\n",
      "| 15|        8|         383|       170|  3563|          10|        70|\n",
      "| 14|        8|         340|       160|  3609|           8|        70|\n",
      "| 15|        8|         400|       150|  3761|         9.5|        70|\n",
      "| 14|        8|         455|       225|  3086|          10|        70|\n",
      "| 24|        4|         113|        95|  2372|          15|        70|\n",
      "| 22|        6|         198|        95|  2833|        15.5|        70|\n",
      "| 18|        6|         199|        97|  2774|        15.5|        70|\n",
      "| 21|        6|         200|        85|  2587|          16|        70|\n",
      "| 27|        4|          97|        88|  2130|        14.5|        70|\n",
      "+---+---------+------------+----------+------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/mpgdata.csv\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal.length: double (nullable = true)\n",
      " |-- sepal.width: double (nullable = true)\n",
      " |-- petal.length: double (nullable = true)\n",
      " |-- petal.width: double (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      "\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal.length|sepal.width|petal.length|petal.width|variety|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| Setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| Setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| Setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| Setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| Setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| Setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| Setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| Setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| Setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| Setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2| Setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2| Setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1| Setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1| Setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2| Setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4| Setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4| Setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3| Setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3| Setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3| Setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"data/iris.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=True, header=True)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON 파일 내용 읽어서 DataFrame 객체 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------------+-------+\n",
      "|     _corrupt_record|            geometry|             properties|   type|\n",
      "+--------------------+--------------------+-----------------------+-------+\n",
      "|                   {|                null|                   null|   null|\n",
      "|\"type\": \"FeatureC...|                null|                   null|   null|\n",
      "|       \"features\": [|                null|                   null|   null|\n",
      "|                null|[[[[127.115195849...|[2013, 11250, 강동구...|Feature|\n",
      "|                null|[[[[127.069069813...|[2013, 11240, 송파구...|Feature|\n",
      "|                null|[[[[127.058673592...|[2013, 11230, 강남구...|Feature|\n",
      "|                null|[[[[127.013971196...|[2013, 11220, 서초구...|Feature|\n",
      "|                null|[[[[126.961089890...|[2013, 11210, 관악구...|Feature|\n",
      "|                null|[[[[126.982238079...|[2013, 11200, 동작구...|Feature|\n",
      "|                null|[[[[126.891846638...|[2013, 11190, 영등포...|Feature|\n",
      "|                null|[[[[126.901560941...|[2013, 11180, 금천구...|Feature|\n",
      "|                null|[[[[126.826880815...|[2013, 11170, 구로구...|Feature|\n",
      "|                null|[[[[126.795757686...|[2013, 11160, 강서구...|Feature|\n",
      "|                null|[[[[126.824233142...|[2013, 11150, 양천구...|Feature|\n",
      "|                null|[[[[126.905220658...|[2013, 11140, 마포구...|Feature|\n",
      "|                null|[[[[126.952475203...|[2013, 11130, 서대문...|Feature|\n",
      "|                null|[[[[126.954675858...|[2013, 11120, 은평구...|Feature|\n",
      "|                null|[[[[127.083875270...|[2013, 11110, 노원구...|Feature|\n",
      "|                null|[[[[127.052884797...|[2013, 11100, 도봉구...|Feature|\n",
      "|                null|[[[[126.993839034...|[2013, 11090, 강북구...|Feature|\n",
      "+--------------------+--------------------+-----------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"data/seoul_geo.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------------+-------+\n",
      "|     _corrupt_record|            geometry|             properties|   type|\n",
      "+--------------------+--------------------+-----------------------+-------+\n",
      "|                   {|                null|                   null|   null|\n",
      "|\"type\": \"FeatureC...|                null|                   null|   null|\n",
      "|       \"features\": [|                null|                   null|   null|\n",
      "|                null|[[[[127.115195849...|[2013, 11250, 강동구...|Feature|\n",
      "|                null|[[[[127.069069813...|[2013, 11240, 송파구...|Feature|\n",
      "|                null|[[[[127.058673592...|[2013, 11230, 강남구...|Feature|\n",
      "|                null|[[[[127.013971196...|[2013, 11220, 서초구...|Feature|\n",
      "|                null|[[[[126.961089890...|[2013, 11210, 관악구...|Feature|\n",
      "|                null|[[[[126.982238079...|[2013, 11200, 동작구...|Feature|\n",
      "|                null|[[[[126.891846638...|[2013, 11190, 영등포...|Feature|\n",
      "|                null|[[[[126.901560941...|[2013, 11180, 금천구...|Feature|\n",
      "|                null|[[[[126.826880815...|[2013, 11170, 구로구...|Feature|\n",
      "|                null|[[[[126.795757686...|[2013, 11160, 강서구...|Feature|\n",
      "|                null|[[[[126.824233142...|[2013, 11150, 양천구...|Feature|\n",
      "|                null|[[[[126.905220658...|[2013, 11140, 마포구...|Feature|\n",
      "|                null|[[[[126.952475203...|[2013, 11130, 서대문...|Feature|\n",
      "|                null|[[[[126.954675858...|[2013, 11120, 은평구...|Feature|\n",
      "|                null|[[[[127.083875270...|[2013, 11110, 노원구...|Feature|\n",
      "|                null|[[[[127.052884797...|[2013, 11100, 도봉구...|Feature|\n",
      "|                null|[[[[126.993839034...|[2013, 11090, 강북구...|Feature|\n",
      "+--------------------+--------------------+-----------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"data/seoul_geo.json\", format=\"json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파케이 파일 내용 읽어서 DataFrame 객체 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n",
      "|first_name|last_name|               email|\n",
      "+----------+---------+--------------------+\n",
      "|    Amanda|   Jordan|    ajordan0@com.com|\n",
      "|    Albert|  Freeman|     afreeman1@is.gd|\n",
      "|    Evelyn|   Morgan|emorgan2@altervis...|\n",
      "|    Denise|    Riley|    driley3@gmpg.org|\n",
      "|    Carlos|    Burns|cburns4@miitbeian...|\n",
      "|   Kathryn|    White|  kwhite5@google.com|\n",
      "|    Samuel|   Holmes|sholmes6@foxnews.com|\n",
      "|     Harry|   Howell| hhowell7@eepurl.com|\n",
      "|      Jose|   Foster|   jfoster8@yelp.com|\n",
      "|     Emily|  Stewart|estewart9@opensou...|\n",
      "|     Susan|  Perkins| sperkinsa@patch.com|\n",
      "|     Alice|    Berry|aberryb@wikipedia...|\n",
      "|    Justin|    Berry|jberryc@usatoday.com|\n",
      "|     Kathy| Reynolds|kreynoldsd@redcro...|\n",
      "|   Dorothy|   Hudson|dhudsone@blogger.com|\n",
      "|     Bruce|   Willis|bwillisf@bluehost...|\n",
      "|     Emily|  Andrews|eandrewsg@cornell...|\n",
      "|   Stephen|  Wallace|swallaceh@netvibe...|\n",
      "|  Clarence|   Lawson|clawsoni@vkontakt...|\n",
      "|   Rebecca|     Bell| rbellj@bandcamp.com|\n",
      "+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"data/userdata1.parquet\")\n",
    "df = df.select(\"first_name\", \"last_name\", \"email\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 직접 만든 DataFrame 객체 생성하여 정보 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|first_name|middle_name|last_name|dob  |gender|salary|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|James     |           |Smith    |36636|M     |60000 |\n",
      "|Michael   |Rose       |         |40288|M     |70000 |\n",
      "|Robert    |           |Williams |42114|      |400000|\n",
      "|Maria     |Anne       |Jones    |39192|F     |500000|\n",
      "|Jen       |Mary       |Brown    |     |F     |0     |\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
    "\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark의 DataFrame 객체를 Pandas의 DataFrame 객체로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "  first_name middle_name last_name    dob gender  salary\n",
      "0      James                 Smith  36636      M   60000\n",
      "1    Michael        Rose            40288      M   70000\n",
      "2     Robert              Williams  42114         400000\n",
      "3      Maria        Anne     Jones  39192      F  500000\n",
      "4        Jen        Mary     Brown             F       0\n"
     ]
    }
   ],
   "source": [
    "pandasDF = pysparkDF.toPandas()\n",
    "print(type(pandasDF))\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+-----+------+----------+----+\n",
      "|empno| ename|  hiredate| sal|\n",
      "+-----+------+----------+----+\n",
      "| 7369| SMITH|1980-12-17| 800|\n",
      "| 7499| ALLEN|1981-02-20|1600|\n",
      "| 7521|  WARD|1981-02-03|1250|\n",
      "| 7566| JONES|1981-03-02|2975|\n",
      "| 7654|MARTIN|1981-10-22|1250|\n",
      "| 7698| BLAKE|1981-05-01|2850|\n",
      "| 7782| CLARK|1981-09-06|2450|\n",
      "| 7788| SCOTT|1982-12-08|3000|\n",
      "| 7839|  KING|1981-11-17|5000|\n",
      "| 7844|TURNER|1984-10-08|1500|\n",
      "| 7876| ADAMS|1983-01-12|1100|\n",
      "| 7900| JAMES|1981-12-03| 950|\n",
      "| 7902|  FORD|1981-12-13|3000|\n",
      "| 7934|MILLER|1982-01-25|1300|\n",
      "+-----+------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp1 = emp.select(\"empno\", \"ename\", \"hiredate\", \"sal\")\n",
    "print(type(emp1))\n",
    "emp1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+----+\n",
      "|empno| ename|  hiredate| sal|\n",
      "+-----+------+----------+----+\n",
      "| 7369| SMITH|1980-12-17| 800|\n",
      "| 7499| ALLEN|1981-02-20|1600|\n",
      "| 7521|  WARD|1981-02-03|1250|\n",
      "| 7566| JONES|1981-03-02|2975|\n",
      "| 7654|MARTIN|1981-10-22|1250|\n",
      "| 7698| BLAKE|1981-05-01|2850|\n",
      "| 7782| CLARK|1981-09-06|2450|\n",
      "| 7788| SCOTT|1982-12-08|3000|\n",
      "| 7839|  KING|1981-11-17|5000|\n",
      "| 7844|TURNER|1984-10-08|1500|\n",
      "| 7876| ADAMS|1983-01-12|1100|\n",
      "| 7900| JAMES|1981-12-03| 950|\n",
      "| 7902|  FORD|1981-12-13|3000|\n",
      "| 7934|MILLER|1982-01-25|1300|\n",
      "+-----+------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.select(emp.empno,emp.ename,emp.hiredate, emp.sal).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`empno`' given input columns: [email, first_name, last_name];;\n'Project ['empno, 'ename, 'hiredate, 'sal]\n+- Project [first_name#514, last_name#515, email#516]\n   +- Relation[registration_dttm#512,id#513,first_name#514,last_name#515,email#516,gender#517,ip_address#518,cc#519,country#520,birthdate#521,salary#522,title#523,comments#524] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-e705698d168f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Using col function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"empno\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ename\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hiredate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m         \"\"\"\n\u001b[1;32m-> 1421\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1422\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`empno`' given input columns: [email, first_name, last_name];;\n'Project ['empno, 'ename, 'hiredate, 'sal]\n+- Project [first_name#514, last_name#515, email#516]\n   +- Relation[registration_dttm#512,id#513,first_name#514,last_name#515,email#516,gender#517,ip_address#518,cc#519,country#520,birthdate#521,salary#522,title#523,comments#524] parquet\n"
     ]
    }
   ],
   "source": [
    "# Using col function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"empno\"),col(\"ename\"),col(\"hiredate\"),col(\"sal\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "----------------------------\n",
      "[Row(empno=7369, ename='SMITH', job='CLERK', mgr=7902, hiredate='1980-12-17', sal=800, comm=None, deptno=20), Row(empno=7499, ename='ALLEN', job='SALESMAN', mgr=7698, hiredate='1981-02-20', sal=1600, comm=300, deptno=30), Row(empno=7521, ename='WARD', job='SALESMAN', mgr=7698, hiredate='1981-02-03', sal=1250, comm=500, deptno=30), Row(empno=7566, ename='JONES', job='MANAGER', mgr=7839, hiredate='1981-03-02', sal=2975, comm=None, deptno=20), Row(empno=7654, ename='MARTIN', job='SALESMAN', mgr=7698, hiredate='1981-10-22', sal=1250, comm=1400, deptno=30), Row(empno=7698, ename='BLAKE', job='MANAGER', mgr=7839, hiredate='1981-05-01', sal=2850, comm=None, deptno=30), Row(empno=7782, ename='CLARK', job='MANAGER', mgr=7839, hiredate='1981-09-06', sal=2450, comm=None, deptno=10), Row(empno=7788, ename='SCOTT', job='ANALYST', mgr=7566, hiredate='1982-12-08', sal=3000, comm=None, deptno=20), Row(empno=7839, ename='KING', job='PRESIDENT', mgr=None, hiredate='1981-11-17', sal=5000, comm=None, deptno=10), Row(empno=7844, ename='TURNER', job='SALESMAN', mgr=7698, hiredate='1984-10-08', sal=1500, comm=None, deptno=30), Row(empno=7876, ename='ADAMS', job='CLERK', mgr=7788, hiredate='1983-01-12', sal=1100, comm=None, deptno=20), Row(empno=7900, ename='JAMES', job='CLERK', mgr=7698, hiredate='1981-12-03', sal=950, comm=None, deptno=30), Row(empno=7902, ename='FORD', job='ANALYST', mgr=7566, hiredate='1981-12-13', sal=3000, comm=None, deptno=20), Row(empno=7934, ename='MILLER', job='CLERK', mgr=7782, hiredate='1982-01-25', sal=1300, comm=None, deptno=10)]\n",
      "----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(empno=7369, ename='SMITH', job='CLERK', mgr=7902, hiredate='1980-12-17', sal=800, comm=None, deptno=20),\n",
       " Row(empno=7499, ename='ALLEN', job='SALESMAN', mgr=7698, hiredate='1981-02-20', sal=1600, comm=300, deptno=30),\n",
       " Row(empno=7521, ename='WARD', job='SALESMAN', mgr=7698, hiredate='1981-02-03', sal=1250, comm=500, deptno=30),\n",
       " Row(empno=7566, ename='JONES', job='MANAGER', mgr=7839, hiredate='1981-03-02', sal=2975, comm=None, deptno=20),\n",
       " Row(empno=7654, ename='MARTIN', job='SALESMAN', mgr=7698, hiredate='1981-10-22', sal=1250, comm=1400, deptno=30),\n",
       " Row(empno=7698, ename='BLAKE', job='MANAGER', mgr=7839, hiredate='1981-05-01', sal=2850, comm=None, deptno=30),\n",
       " Row(empno=7782, ename='CLARK', job='MANAGER', mgr=7839, hiredate='1981-09-06', sal=2450, comm=None, deptno=10),\n",
       " Row(empno=7788, ename='SCOTT', job='ANALYST', mgr=7566, hiredate='1982-12-08', sal=3000, comm=None, deptno=20),\n",
       " Row(empno=7839, ename='KING', job='PRESIDENT', mgr=None, hiredate='1981-11-17', sal=5000, comm=None, deptno=10),\n",
       " Row(empno=7844, ename='TURNER', job='SALESMAN', mgr=7698, hiredate='1984-10-08', sal=1500, comm=None, deptno=30),\n",
       " Row(empno=7876, ename='ADAMS', job='CLERK', mgr=7788, hiredate='1983-01-12', sal=1100, comm=None, deptno=20),\n",
       " Row(empno=7900, ename='JAMES', job='CLERK', mgr=7698, hiredate='1981-12-03', sal=950, comm=None, deptno=30),\n",
       " Row(empno=7902, ename='FORD', job='ANALYST', mgr=7566, hiredate='1981-12-13', sal=3000, comm=None, deptno=20),\n",
       " Row(empno=7934, ename='MILLER', job='CLERK', mgr=7782, hiredate='1982-01-25', sal=1300, comm=None, deptno=10)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataCollect = emp.collect()\n",
    "print(type(dataCollect))\n",
    "print(\"----------------------------\")\n",
    "print(dataCollect)\n",
    "print(\"----------------------------\")\n",
    "display(dataCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: integer (nullable = true)\n",
      " |-- hiredate: string (nullable = true)\n",
      " |-- sal: integer (nullable = true)\n",
      " |-- comm: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: integer (nullable = true)\n",
      " |-- hiredate: string (nullable = true)\n",
      " |-- sal: integer (nullable = true)\n",
      " |-- comm: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp = emp.withColumn(\"deptno\",col(\"deptno\").cast(\"Integer\"))\n",
    "newemp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+------+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate|   sal|comm|deptno|\n",
      "+-----+------+---------+----+----------+------+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 80000|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|160000| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03|125000| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02|297500|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22|125000|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|285000|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06|245000|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08|300000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|500000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08|150000|null|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12|110000|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 95000|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13|300000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25|130000|null|    10|\n",
      "+-----+------+---------+----+----------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp = newemp.withColumn(\"sal\",col(\"sal\")*100)\n",
    "newemp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## withColumnRenamed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+------+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate|salary|comm|deptno|\n",
      "+-----+------+---------+----+----------+------+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 80000|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|160000| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03|125000| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02|297500|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22|125000|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|285000|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06|245000|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08|300000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|500000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08|150000|null|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12|110000|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 95000|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13|300000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25|130000|null|    10|\n",
      "+-----+------+---------+----+----------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp = newemp.withColumnRenamed(\"sal\",\"salary\")\n",
    "newemp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+-------+----------+------+----+------+\n",
      "|empno|empname|      job|manager|  hiredate|salary|comm|deptno|\n",
      "+-----+-------+---------+-------+----------+------+----+------+\n",
      "| 7369|  SMITH|    CLERK|   7902|1980-12-17| 80000|null|    20|\n",
      "| 7499|  ALLEN| SALESMAN|   7698|1981-02-20|160000| 300|    30|\n",
      "| 7521|   WARD| SALESMAN|   7698|1981-02-03|125000| 500|    30|\n",
      "| 7566|  JONES|  MANAGER|   7839|1981-03-02|297500|null|    20|\n",
      "| 7654| MARTIN| SALESMAN|   7698|1981-10-22|125000|1400|    30|\n",
      "| 7698|  BLAKE|  MANAGER|   7839|1981-05-01|285000|null|    30|\n",
      "| 7782|  CLARK|  MANAGER|   7839|1981-09-06|245000|null|    10|\n",
      "| 7788|  SCOTT|  ANALYST|   7566|1982-12-08|300000|null|    20|\n",
      "| 7839|   KING|PRESIDENT|   null|1981-11-17|500000|null|    10|\n",
      "| 7844| TURNER| SALESMAN|   7698|1984-10-08|150000|null|    30|\n",
      "| 7876|  ADAMS|    CLERK|   7788|1983-01-12|110000|null|    20|\n",
      "| 7900|  JAMES|    CLERK|   7698|1981-12-03| 95000|null|    30|\n",
      "| 7902|   FORD|  ANALYST|   7566|1981-12-13|300000|null|    20|\n",
      "| 7934| MILLER|    CLERK|   7782|1982-01-25|130000|null|    10|\n",
      "+-----+-------+---------+-------+----------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp = newemp.withColumnRenamed(\"mgr\",\"manager\") \\\n",
    "    .withColumnRenamed(\"ename\",\"empname\")\n",
    "newemp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter() - where() 와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "|empno|ename|job      |mgr |hiredate  |sal |comm|deptno|\n",
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "|7839 |KING |PRESIDENT|null|1981-11-17|5000|null|10    |\n",
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.filter(emp.ename == \"KING\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "|empno|ename|job      |mgr |hiredate  |sal |comm|deptno|\n",
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "|7839 |KING |PRESIDENT|null|1981-11-17|5000|null|10    |\n",
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.filter('ename == \"KING\"').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+----+----------+----+----+------+\n",
      "|empno|ename |job     |mgr |hiredate  |sal |comm|deptno|\n",
      "+-----+------+--------+----+----------+----+----+------+\n",
      "|7499 |ALLEN |SALESMAN|7698|1981-02-20|1600|300 |30    |\n",
      "|7698 |BLAKE |MANAGER |7839|1981-05-01|2850|null|30    |\n",
      "|7844 |TURNER|SALESMAN|7698|1984-10-08|1500|null|30    |\n",
      "+-----+------+--------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.filter((emp.deptno == 30) & (emp.sal >= 1500)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+----+----------+----+----+------+\n",
      "|empno|ename |job     |mgr |hiredate  |sal |comm|deptno|\n",
      "+-----+------+--------+----+----------+----+----+------+\n",
      "|7499 |ALLEN |SALESMAN|7698|1981-02-20|1600|300 |30    |\n",
      "|7698 |BLAKE |MANAGER |7839|1981-05-01|2850|null|30    |\n",
      "|7844 |TURNER|SALESMAN|7698|1984-10-08|1500|null|30    |\n",
      "+-----+------+--------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.where((emp.deptno == 30) & (emp.sal >= 1500)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop (), distinct(), dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|      job|deptno|\n",
      "+---------+------+\n",
      "|    CLERK|    20|\n",
      "| SALESMAN|    30|\n",
      "| SALESMAN|    30|\n",
      "|  MANAGER|    20|\n",
      "| SALESMAN|    30|\n",
      "|  MANAGER|    30|\n",
      "|  MANAGER|    10|\n",
      "|  ANALYST|    20|\n",
      "|PRESIDENT|    10|\n",
      "| SALESMAN|    30|\n",
      "|    CLERK|    20|\n",
      "|    CLERK|    30|\n",
      "|  ANALYST|    20|\n",
      "|    CLERK|    10|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empnew = emp.select(\"job\", \"deptno\")\n",
    "empnew.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|      job|deptno|\n",
      "+---------+------+\n",
      "|  ANALYST|    20|\n",
      "|  MANAGER|    10|\n",
      "|  MANAGER|    30|\n",
      "|PRESIDENT|    10|\n",
      "|    CLERK|    20|\n",
      "| SALESMAN|    30|\n",
      "|    CLERK|    10|\n",
      "|  MANAGER|    20|\n",
      "|    CLERK|    30|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empnew.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|      job|deptno|\n",
      "+---------+------+\n",
      "|  ANALYST|    20|\n",
      "|  MANAGER|    10|\n",
      "|  MANAGER|    30|\n",
      "|PRESIDENT|    10|\n",
      "|    CLERK|    20|\n",
      "| SALESMAN|    30|\n",
      "|    CLERK|    10|\n",
      "|  MANAGER|    20|\n",
      "|    CLERK|    30|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empnew.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      job|\n",
      "+---------+\n",
      "|    CLERK|\n",
      "| SALESMAN|\n",
      "| SALESMAN|\n",
      "|  MANAGER|\n",
      "| SALESMAN|\n",
      "|  MANAGER|\n",
      "|  MANAGER|\n",
      "|  ANALYST|\n",
      "|PRESIDENT|\n",
      "| SALESMAN|\n",
      "|    CLERK|\n",
      "|    CLERK|\n",
      "|  ANALYST|\n",
      "|    CLERK|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empnew.drop(\"deptno\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## orderBy(), sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno|ename |job      |mgr |hiredate  |sal |comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17|800 |null|20    |\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03|950 |null|30    |\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12|1100|null|20    |\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22|1250|1400|30    |\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03|1250|500 |30    |\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25|1300|null|10    |\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08|1500|null|30    |\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20|1600|300 |30    |\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06|2450|null|10    |\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01|2850|null|30    |\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02|2975|null|20    |\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08|3000|null|20    |\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13|3000|null|20    |\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17|5000|null|10    |\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.sort(\"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno|ename |job      |mgr |hiredate  |sal |comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17|5000|null|10    |\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08|3000|null|20    |\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13|3000|null|20    |\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02|2975|null|20    |\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01|2850|null|30    |\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06|2450|null|10    |\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20|1600|300 |30    |\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08|1500|null|30    |\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25|1300|null|10    |\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22|1250|1400|30    |\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03|1250|500 |30    |\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12|1100|null|20    |\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03|950 |null|30    |\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17|800 |null|20    |\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.sort(emp.sal.desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno|ename |job      |mgr |hiredate  |sal |comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25|1300|null|10    |\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06|2450|null|10    |\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17|5000|null|10    |\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17|800 |null|20    |\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12|1100|null|20    |\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02|2975|null|20    |\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13|3000|null|20    |\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08|3000|null|20    |\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03|950 |null|30    |\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22|1250|1400|30    |\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03|1250|500 |30    |\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08|1500|null|30    |\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20|1600|300 |30    |\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01|2850|null|30    |\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.sort(\"deptno\", \"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno|ename |job      |mgr |hiredate  |sal |comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01|2850|null|30    |\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20|1600|300 |30    |\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08|1500|null|30    |\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22|1250|1400|30    |\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03|1250|500 |30    |\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03|950 |null|30    |\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13|3000|null|20    |\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08|3000|null|20    |\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02|2975|null|20    |\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12|1100|null|20    |\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17|800 |null|20    |\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17|5000|null|10    |\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06|2450|null|10    |\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25|1300|null|10    |\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.sort(emp.deptno.desc(), emp.sal.desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno|ename |job      |mgr |hiredate  |sal |comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01|2850|null|30    |\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20|1600|300 |30    |\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08|1500|null|30    |\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22|1250|1400|30    |\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03|1250|500 |30    |\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03|950 |null|30    |\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13|3000|null|20    |\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08|3000|null|20    |\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02|2975|null|20    |\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12|1100|null|20    |\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17|800 |null|20    |\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17|5000|null|10    |\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06|2450|null|10    |\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25|1300|null|10    |\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.orderBy(emp.deptno.desc(), emp.sal.desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno|ename |job      |mgr |hiredate  |sal |comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17|800 |null|20    |\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03|1250|500 |30    |\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20|1600|300 |30    |\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02|2975|null|20    |\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01|2850|null|30    |\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06|2450|null|10    |\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22|1250|1400|30    |\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17|5000|null|10    |\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03|950 |null|30    |\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13|3000|null|20    |\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25|1300|null|10    |\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08|3000|null|20    |\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12|1100|null|20    |\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08|1500|null|30    |\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno|ename |job      |mgr |hiredate  |sal |comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17|800 |null|20    |\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03|1250|500 |30    |\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20|1600|300 |30    |\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02|2975|null|20    |\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01|2850|null|30    |\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06|2450|null|10    |\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22|1250|1400|30    |\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17|5000|null|10    |\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03|950 |null|30    |\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13|3000|null|20    |\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25|1300|null|10    |\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08|3000|null|20    |\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12|1100|null|20    |\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08|1500|null|30    |\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.sort(col(\"hiredate\").asc(),col(\"sal\").asc()).show(truncate=False)\n",
    "emp.orderBy(col(\"hiredate\").asc(),col(\"sal\").asc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|deptno|sum(sal)|\n",
      "+------+--------+\n",
      "|20    |10875   |\n",
      "|10    |8750    |\n",
      "|30    |9400    |\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\").sum(\"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|deptno|min(sal)|\n",
      "+------+--------+\n",
      "|20    |800     |\n",
      "|10    |1300    |\n",
      "|30    |950     |\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\").min(\"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|deptno|max(sal)|\n",
      "+------+--------+\n",
      "|20    |3000    |\n",
      "|10    |5000    |\n",
      "|30    |2850    |\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\").max(\"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|deptno|avg(sal)          |\n",
      "+------+------------------+\n",
      "|20    |2175.0            |\n",
      "|10    |2916.6666666666665|\n",
      "|30    |1566.6666666666667|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\").avg(\"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|deptno|avg(sal)          |\n",
      "+------+------------------+\n",
      "|20    |2175.0            |\n",
      "|10    |2916.6666666666665|\n",
      "|30    |1566.6666666666667|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\").avg(\"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+\n",
      "|deptno|job      |sum(sal)|\n",
      "+------+---------+--------+\n",
      "|20    |ANALYST  |6000    |\n",
      "|20    |MANAGER  |2975    |\n",
      "|30    |MANAGER  |2850    |\n",
      "|30    |SALESMAN |5600    |\n",
      "|30    |CLERK    |950     |\n",
      "|20    |CLERK    |1900    |\n",
      "|10    |PRESIDENT|5000    |\n",
      "|10    |CLERK    |1300    |\n",
      "|10    |MANAGER  |2450    |\n",
      "+------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\", \"job\").sum(\"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+\n",
      "|deptno|sum(sal)|sum(comm)|\n",
      "+------+--------+---------+\n",
      "|20    |10875   |null     |\n",
      "|10    |8750    |null     |\n",
      "|30    |9400    |2200     |\n",
      "+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\").sum(\"sal\", \"comm\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+--------+--------+------------------+\n",
      "|deptno|sum(sal)|avg(sal)          |max(sal)|min(sal)|avg(sal)          |\n",
      "+------+--------+------------------+--------+--------+------------------+\n",
      "|20    |10875   |2175.0            |3000    |800     |2175.0            |\n",
      "|10    |8750    |2916.6666666666665|5000    |1300    |2916.6666666666665|\n",
      "|30    |9400    |1566.6666666666667|2850    |950     |1566.6666666666667|\n",
      "+------+--------+------------------+--------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,min,mean,count\n",
    "emp.groupBy(\"deptno\").agg(sum(\"sal\"), avg(\"sal\"), max(\"sal\"), min(\"sal\"), mean(\"sal\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------------+----------+----------+------------------+\n",
      "|deptno|sum_salary|avg_salary        |max_salary|min_salary|mean_salary       |\n",
      "+------+----------+------------------+----------+----------+------------------+\n",
      "|20    |10875     |2175.0            |3000      |800       |2175.0            |\n",
      "|10    |8750      |2916.6666666666665|5000      |1300      |2916.6666666666665|\n",
      "|30    |9400      |1566.6666666666667|2850      |950       |1566.6666666666667|\n",
      "+------+----------+------------------+----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\") \\\n",
    "    .agg(sum(\"sal\").alias(\"sum_salary\"), \\\n",
    "         avg(\"sal\").alias(\"avg_salary\"), \\\n",
    "         max(\"sal\").alias(\"max_salary\"), \\\n",
    "         min(\"sal\").alias(\"min_salary\"), \\\n",
    "         mean(\"sal\").alias(\"mean_salary\"), \\\n",
    "     ) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------------+----------+----------+------------------+\n",
      "|deptno|sum_salary|avg_salary        |max_salary|min_salary|mean_salary       |\n",
      "+------+----------+------------------+----------+----------+------------------+\n",
      "|20    |10875     |2175.0            |3000      |800       |2175.0            |\n",
      "|30    |9400      |1566.6666666666667|2850      |950       |1566.6666666666667|\n",
      "+------+----------+------------------+----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\") \\\n",
    "    .agg(sum(\"sal\").alias(\"sum_salary\"), \\\n",
    "         avg(\"sal\").alias(\"avg_salary\"), \\\n",
    "         max(\"sal\").alias(\"max_salary\"), \\\n",
    "         min(\"sal\").alias(\"min_salary\"), \\\n",
    "         mean(\"sal\").alias(\"mean_salary\"), \\\n",
    "     ) \\\n",
    "    .where(col(\"sum_salary\") > 9000)\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+\n",
      "|deptno|dname   |loc |\n",
      "+------+--------+----+\n",
      "|10    |영업부  |서울|\n",
      "|20    |개발부  |대전|\n",
      "|30    |기획부  |서울|\n",
      "|40    |마케팅부|서울|\n",
      "+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptdata = [(10, '영업부', '서울'), (20, '개발부', '대전'), (30, '기획부', '서울'), (40, '마케팅부', '서울')]\n",
    "deptcolname = ['deptno', 'dname', 'loc']\n",
    "dept = spark.createDataFrame(data=deptdata, schema=deptcolname)\n",
    "dept.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+------+------+----+\n",
      "|empno|ename |job      |mgr |hiredate  |sal |comm|deptno|deptno|dname |loc |\n",
      "+-----+------+---------+----+----------+----+----+------+------+------+----+\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25|1300|null|10    |10    |영업부|서울|\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17|5000|null|10    |10    |영업부|서울|\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06|2450|null|10    |10    |영업부|서울|\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13|3000|null|20    |20    |개발부|대전|\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12|1100|null|20    |20    |개발부|대전|\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08|3000|null|20    |20    |개발부|대전|\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02|2975|null|20    |20    |개발부|대전|\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17|800 |null|20    |20    |개발부|대전|\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03|950 |null|30    |30    |기획부|서울|\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08|1500|null|30    |30    |기획부|서울|\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01|2850|null|30    |30    |기획부|서울|\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22|1250|1400|30    |30    |기획부|서울|\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03|1250|500 |30    |30    |기획부|서울|\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20|1600|300 |30    |30    |기획부|서울|\n",
      "+-----+------+---------+----+----------+----+----+------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.join(dept,emp.deptno ==  dept.deptno,\"inner\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+------+--------+----+\n",
      "|empno|ename |job      |mgr |hiredate  |sal |comm|deptno|deptno|dname   |loc |\n",
      "+-----+------+---------+----+----------+----+----+------+------+--------+----+\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25|1300|null|10    |10    |영업부  |서울|\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17|5000|null|10    |10    |영업부  |서울|\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06|2450|null|10    |10    |영업부  |서울|\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13|3000|null|20    |20    |개발부  |대전|\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12|1100|null|20    |20    |개발부  |대전|\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08|3000|null|20    |20    |개발부  |대전|\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02|2975|null|20    |20    |개발부  |대전|\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17|800 |null|20    |20    |개발부  |대전|\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03|950 |null|30    |30    |기획부  |서울|\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08|1500|null|30    |30    |기획부  |서울|\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01|2850|null|30    |30    |기획부  |서울|\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22|1250|1400|30    |30    |기획부  |서울|\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03|1250|500 |30    |30    |기획부  |서울|\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20|1600|300 |30    |30    |기획부  |서울|\n",
      "|null |null  |null     |null|null      |null|null|null  |40    |마케팅부|서울|\n",
      "+-----+------+---------+----+----------+----+----+------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.join(dept,emp.deptno ==  dept.deptno,\"right\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## union()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|ename| sal|\n",
      "+-----+----+\n",
      "|JONES|2975|\n",
      "|BLAKE|2850|\n",
      "|CLARK|2450|\n",
      "+-----+----+\n",
      "\n",
      "+------+----+\n",
      "| ename| sal|\n",
      "+------+----+\n",
      "| ALLEN|1600|\n",
      "|  WARD|1250|\n",
      "|MARTIN|1250|\n",
      "| BLAKE|2850|\n",
      "|TURNER|1500|\n",
      "| JAMES| 950|\n",
      "+------+----+\n",
      "\n",
      "+------+----+\n",
      "| ename| sal|\n",
      "+------+----+\n",
      "| JONES|2975|\n",
      "| BLAKE|2850|\n",
      "| CLARK|2450|\n",
      "| ALLEN|1600|\n",
      "|  WARD|1250|\n",
      "|MARTIN|1250|\n",
      "| BLAKE|2850|\n",
      "|TURNER|1500|\n",
      "| JAMES| 950|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp1 = emp.filter(\"job == 'MANAGER'\").select(\"ename\", \"sal\")\n",
    "emp2 = emp.filter(\"deptno == 30\").select(\"ename\", \"sal\")\n",
    "emp1.show()\n",
    "emp2.show()\n",
    "emp1.union(emp2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|ename| sal|\n",
      "+-----+----+\n",
      "|JONES|2975|\n",
      "|BLAKE|2850|\n",
      "|CLARK|2450|\n",
      "+-----+----+\n",
      "\n",
      "+------+----+\n",
      "| ename| sal|\n",
      "+------+----+\n",
      "| ALLEN|1600|\n",
      "|  WARD|1250|\n",
      "|MARTIN|1250|\n",
      "| BLAKE|2850|\n",
      "|TURNER|1500|\n",
      "| JAMES| 950|\n",
      "+------+----+\n",
      "\n",
      "+------+----+\n",
      "| ename| sal|\n",
      "+------+----+\n",
      "| BLAKE|2850|\n",
      "|MARTIN|1250|\n",
      "|TURNER|1500|\n",
      "| CLARK|2450|\n",
      "| JAMES| 950|\n",
      "| ALLEN|1600|\n",
      "| JONES|2975|\n",
      "|  WARD|1250|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp1 = emp.filter(\"job == 'MANAGER'\").select(\"ename\", \"sal\")\n",
    "emp2 = emp.filter(\"deptno == 30\").select(\"ename\", \"sal\")\n",
    "emp1.show()\n",
    "emp2.show()\n",
    "emp1.union(emp2).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **map() 과 flatMap()**\n",
    "\n",
    "### lines = [['w1',  'w2', 'w3'], ['w4', 'w5', 'w6']]\n",
    "### lines를 map/flatmap을 이용하여 split하게 되면 아래와 같다.\n",
    "### map: one2one mapping\n",
    "###\tArray(Array('w1', 'w2', 'w3'), Array('w4', 'w5', 'w6'))\n",
    "\n",
    "### flatmap: one example → one result(flatten)\n",
    "### Array('w1', 'w2', 'w3', 'w4', 'w5', 'w6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "둘리 또치 도우너 희동이 고길동 마이콜\n",
      "피카츄 꼬부기 잠만보\n",
      "듀크 턱시\n",
      "프로도 간달프 스미골\n",
      "코코\n"
     ]
    }
   ],
   "source": [
    "data = [\"둘리 또치 도우너 희동이 고길동 마이콜\",\n",
    "        \"피카츄 꼬부기 잠만보\",\n",
    "        \"듀크 턱시\",\n",
    "        \"프로도 간달프 스미골\",\n",
    "        \"코코\"]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "for element in rdd.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['둘리', '또치', '도우너', '희동이', '고길동', '마이콜'],\n",
       " ['피카츄', '꼬부기', '잠만보'],\n",
       " ['듀크', '턱시'],\n",
       " ['프로도', '간달프', '스미골'],\n",
       " ['코코']]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2=rdd.map(lambda x: x.split(\" \"))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['둘리',\n",
       " '또치',\n",
       " '도우너',\n",
       " '희동이',\n",
       " '고길동',\n",
       " '마이콜',\n",
       " '피카츄',\n",
       " '꼬부기',\n",
       " '잠만보',\n",
       " '듀크',\n",
       " '턱시',\n",
       " '프로도',\n",
       " '간달프',\n",
       " '스미골',\n",
       " '코코']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[range(1, 3), range(1, 4), range(1, 5)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([3,4,5]).map(lambda x: range(1,x)).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 2, 3, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([3,4,5]).flatMap(lambda x: range(1,x)).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 9], [4, 16], [5, 25]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([3,4,5]).map(lambda x: [x,  x*x]).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 4, 16, 5, 25]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([3,4,5]).flatMap(lambda x: [x,  x*x]).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Birthday', 1),\n",
       " ('Day', 1),\n",
       " ('Evening', 1),\n",
       " ('Good', 3),\n",
       " ('Happy', 2),\n",
       " ('Morning', 1),\n",
       " ('New', 1),\n",
       " ('Year', 1)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.sparkContext.textFile(\"data/greeting.txt\")\n",
    "sorted(lines.flatMap(lambda line: line.split()).map(lambda w: (w,1)).reduceByKey(lambda v1, v2: v1+v2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "data/greeting.txt MapPartitionsRDD[374] at textFile at NativeMethodAccessorImpl.java:0\n",
      "['Good Morning', 'Good Evening', 'Good Day', 'Happy Birthday', 'Happy New Year']\n",
      "------------------------------------------------------------------------------\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[375] at RDD at PythonRDD.scala:53\n",
      "['Good', 'Morning', 'Good', 'Evening', 'Good', 'Day', 'Happy', 'Birthday', 'Happy', 'New', 'Year']\n",
      "------------------------------------------------------------------------------\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[376] at RDD at PythonRDD.scala:53\n",
      "[('Good', 1), ('Morning', 1), ('Good', 1), ('Evening', 1), ('Good', 1), ('Day', 1), ('Happy', 1), ('Birthday', 1), ('Happy', 1), ('New', 1), ('Year', 1)]\n",
      "------------------------------------------------------------------------------\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[381] at RDD at PythonRDD.scala:53\n",
      "[('Good', 3), ('Morning', 1), ('Evening', 1), ('Birthday', 1), ('New', 1), ('Year', 1), ('Day', 1), ('Happy', 2)]\n",
      "------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "[('Good', 3), ('Morning', 1), ('Evening', 1), ('Birthday', 1), ('New', 1), ('Year', 1), ('Day', 1), ('Happy', 2)]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"data/greeting.txt\")\n",
    "print(type(rdd1))\n",
    "print(rdd1)\n",
    "print(rdd1.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "rdd2 = rdd1.flatMap(lambda line: line.split())\n",
    "print(type(rdd2))\n",
    "print(rdd2)\n",
    "print(rdd2.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "rdd3 = rdd2.map(lambda w: (w,1))\n",
    "print(type(rdd3))\n",
    "print(rdd3)      \n",
    "print(rdd3.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "rdd4 = rdd3.reduceByKey(lambda v1, v2: v1+v2)\n",
    "print(type(rdd4))\n",
    "print(rdd4)\n",
    "print(rdd4.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "result = rdd4.collect()\n",
    "print(type(result))\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydatavenv",
   "language": "python",
   "name": "pydatavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
