{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [PySpark API 도큐먼트](https://spark.apache.org/docs/latest/api/python/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **pyspark 패키지를 활용한 Spark 프로그래밍(2)**\n",
    "## SparkSession 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://121.133.223.69:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sparkedu</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x166bf81e3a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[2]\") \\\n",
    "                    .appName('sparkedu') \\\n",
    "                    .getOrCreate()\n",
    "spark\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:red'>**RDD**</span>\n",
    "### Resilient Distributed Dataset의 약자(탄력 분산 데이터셋)\n",
    "### 분산되어 존재하는 데이터들의 모임, 즉 클러스터에 분배되어 있는 데이터들을 하나로 관리하는 개념\n",
    "### 스파크의 모든 데이터 타입들은 RDD를 기반으로 만들어지고 데이터끼리의 연산들은 RDD의 연산으로 이루어져 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/greeting.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Good Morning',\n",
       " 'Good Evening',\n",
       " 'Good Day',\n",
       " 'Happy Birthday',\n",
       " 'Happy New Year']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greetRDD = spark.sparkContext.textFile('data/greeting.txt')\n",
    "print(greetRDD)\n",
    "greetRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good Morning', 'Good Evening', 'Good Day']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goodLines = greetRDD.filter(lambda x : \"Good\" in x)\n",
    "goodLines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goodLines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = spark.sparkContext.parallelize(list(range(5)))\n",
    "squared = numbers.map(lambda x : x * x).collect()\n",
    "squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'spark', 'hi', 'python']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = spark.sparkContext.parallelize([\"hello spark\", \"hi python\"])\n",
    "splitted = strings.flatMap(lambda x : x.split(\" \")).collect()\n",
    "splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 10, 16, 22, 28]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = spark.sparkContext.parallelize(list(range(1, 30, 3)))\n",
    "result = numbers.filter(lambda x : x % 2 == 0).collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[10] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesRDD = spark.sparkContext.parallelize([\"test\", \"this is a test rdd\"])\n",
    "linesRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:red'>**페어 RDD**</span>\n",
    "### 페어 RDD란 key-value쌍으로 이루어진 RDD\n",
    "### 파이썬에서는 Tuple로 이뤄진 RDD가 곧 페어 RDD가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[11] at readRDDFromFile at PythonRDD.scala:262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 3), (1, 5), (2, 4), (3, 3), (4, 8), (4, 2), (3, 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examplePairRDD = spark.sparkContext.parallelize([(1, 3), (1, 5), (2, 4), (3, 3), (4, 8), (4, 2), (3, 1)])\n",
    "print(examplePairRDD)\n",
    "examplePairRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reduceByKey(func) : 동일 키에 대한 값들을 reduce(예 : rdd.reduceByKey(lambda x, y: x + y))\n",
    "- mapValues(func) : 각 키에 대해 연산을 적용(예 : rdd.mapValues(lambda x : x + 1))\n",
    "- sortByKey() : 키로 정렬한 RDD 리턴(예 : rdd.sortByKey())\n",
    "- keys() : 키값들을 리턴(예 : rdd.keys())\n",
    "- values() : value값들을 리턴(예 : rdd.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 4), (4, 10), (1, 8), (3, 4)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examplePairRDD.reduceByKey(lambda x, y : x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 9), (1, 25), (2, 16), (3, 9), (4, 64), (4, 4), (3, 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examplePairRDD.mapValues(lambda x: x**2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alfreds Futterkiste,Germany'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerLines = spark.sparkContext.textFile(\"data/name-customers.csv\")\n",
    "customerLines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[106] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerPairs = customerLines.map(lambda x: (x.split(\",\")[1], x.split(\",\")[0]))\n",
    "customerPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Around the Horn',\n",
       " \"B's Beverages\",\n",
       " 'Consolidated Holdings',\n",
       " 'Eastern Connection',\n",
       " 'Island Trading',\n",
       " 'North/South',\n",
       " 'Seven Seas Imports']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerPairCollected = customerPairs.groupByKey().collect()\n",
    "customerDict = {\n",
    "    country : [c for c in customers]\n",
    "    for country, customers in customerPairCollected\n",
    "}\n",
    "customerDict['UK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Argentina',\n",
       " 'Argentina',\n",
       " 'Argentina',\n",
       " 'Austria',\n",
       " 'Austria',\n",
       " 'Belgium',\n",
       " 'Belgium',\n",
       " 'Brazil',\n",
       " 'Brazil',\n",
       " 'Brazil']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in customerPairs.sortByKey().keys().collect()][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mexico': 5,\n",
       " 'France': 11,\n",
       " 'Argentina': 3,\n",
       " 'Switzerland': 2,\n",
       " 'Brazil': 9,\n",
       " 'Austria': 2,\n",
       " 'Portugal': 2,\n",
       " 'USA': 13,\n",
       " 'Venezuela': 4,\n",
       " 'Ireland': 1,\n",
       " 'Belgium': 2,\n",
       " 'Norway': 1,\n",
       " 'Denmark': 2,\n",
       " 'Finland': 2,\n",
       " 'Poland': 1,\n",
       " 'Germany': 11,\n",
       " 'UK': 7,\n",
       " 'Sweden': 2,\n",
       " 'Spain': 5,\n",
       " 'Canada': 3,\n",
       " 'Italy': 3}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapReduced = customerPairs.mapValues(lambda x : 1).reduceByKey(lambda x, y: x + y)\n",
    "{\n",
    "    i:j for i, j in mapReduced.collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD를 가지고 워드카운팅하는 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Birthday', 1),\n",
       " ('Day', 1),\n",
       " ('Evening', 1),\n",
       " ('Good', 3),\n",
       " ('Happy', 2),\n",
       " ('Morning', 1),\n",
       " ('New', 1),\n",
       " ('Year', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.sparkContext.textFile(\"data/greeting.txt\")\n",
    "sorted(lines.flatMap(lambda line: line.split()).map(lambda w: (w,1)).reduceByKey(lambda v1, v2: v1+v2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "data/greeting.txt MapPartitionsRDD[30] at textFile at NativeMethodAccessorImpl.java:0\n",
      "['Good Morning', 'Good Evening', 'Good Day', 'Happy Birthday', 'Happy New Year']\n",
      "------------------------------------------------------------------------------\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[31] at RDD at PythonRDD.scala:53\n",
      "['Good', 'Morning', 'Good', 'Evening', 'Good', 'Day', 'Happy', 'Birthday', 'Happy', 'New', 'Year']\n",
      "------------------------------------------------------------------------------\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[32] at RDD at PythonRDD.scala:53\n",
      "[('Good', 1), ('Morning', 1), ('Good', 1), ('Evening', 1), ('Good', 1), ('Day', 1), ('Happy', 1), ('Birthday', 1), ('Happy', 1), ('New', 1), ('Year', 1)]\n",
      "------------------------------------------------------------------------------\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[37] at RDD at PythonRDD.scala:53\n",
      "[('Good', 3), ('Morning', 1), ('Evening', 1), ('Birthday', 1), ('New', 1), ('Year', 1), ('Day', 1), ('Happy', 2)]\n",
      "------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "[('Good', 3), ('Morning', 1), ('Evening', 1), ('Birthday', 1), ('New', 1), ('Year', 1), ('Day', 1), ('Happy', 2)]\n",
      "------------------------------------------------------------------------------\n",
      "[('Birthday', 1), ('Day', 1), ('Evening', 1), ('Good', 3), ('Happy', 2), ('Morning', 1), ('New', 1), ('Year', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"data/greeting.txt\")\n",
    "print(type(rdd1))\n",
    "print(rdd1)\n",
    "print(rdd1.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "rdd2 = rdd1.flatMap(lambda line: line.split())\n",
    "print(type(rdd2))\n",
    "print(rdd2)\n",
    "print(rdd2.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "rdd3 = rdd2.map(lambda w: (w,1))\n",
    "print(type(rdd3))\n",
    "print(rdd3)      \n",
    "print(rdd3.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "rdd4 = rdd3.reduceByKey(lambda v1, v2: v1+v2)\n",
    "print(type(rdd4))\n",
    "print(rdd4)\n",
    "print(rdd4.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "result = rdd4.collect()\n",
    "print(type(result))\n",
    "print(result)\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(sorted(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일 로딩(JSON, CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[40] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "carsJson = spark.sparkContext.textFile(\"./data/cars.json\")\\\n",
    "              .map(lambda x: json.loads(x))\n",
    "carsJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brand': 'Ford', 'models': {'name': 'Fiesta', 'price': '14260'}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carsJson.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'brand': 'Ford', 'models': {'name': 'Fiesta', 'price': '14260'}},\n",
       " {'brand': 'Ford', 'models': {'name': 'Focus', 'price': '18825'}},\n",
       " {'brand': 'Ford', 'models': {'name': 'Mustang', 'price': '26670'}},\n",
       " {'brand': 'BMW', 'models': {'name': '320', 'price': '40250'}},\n",
       " {'brand': 'BMW', 'models': {'name': 'X3', 'price': '41000'}},\n",
       " {'brand': 'BMW', 'models': {'name': 'X5', 'price': '60700'}},\n",
       " {'brand': 'Fiat', 'models': {'name': '500', 'price': '16495'}}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carsJson.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD를 가지고 Hive가상테이블 생성 ~> SQL을 사용해서 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = spark.read.csv(\"data/emp.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "hiveCtx = HiveContext(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[empno: int, ename: string, job: string, mgr: int, hiredate: string, sal: int, comm: int, deptno: int]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.registerTempTable(\"hiveemp\")\n",
    "emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ename='SMITH', sal=800),\n",
       " Row(ename='ALLEN', sal=1600),\n",
       " Row(ename='WARD', sal=1250),\n",
       " Row(ename='JONES', sal=2975),\n",
       " Row(ename='MARTIN', sal=1250)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empResult = hiveCtx.sql(\"SELECT ename, sal FROM hiveemp\")\n",
    "empResult.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(empno=7369, ename='SMITH', job='CLERK', mgr=7902, hiredate='1980-12-17', sal=800, comm=None, deptno=20),\n",
       " Row(empno=7900, ename='JAMES', job='CLERK', mgr=7698, hiredate='1981-12-03', sal=950, comm=None, deptno=30),\n",
       " Row(empno=7876, ename='ADAMS', job='CLERK', mgr=7788, hiredate='1983-01-12', sal=1100, comm=None, deptno=20),\n",
       " Row(empno=7521, ename='WARD', job='SALESMAN', mgr=7698, hiredate='1981-02-03', sal=1250, comm=500, deptno=30),\n",
       " Row(empno=7654, ename='MARTIN', job='SALESMAN', mgr=7698, hiredate='1981-10-22', sal=1250, comm=1400, deptno=30),\n",
       " Row(empno=7934, ename='MILLER', job='CLERK', mgr=7782, hiredate='1982-01-25', sal=1300, comm=None, deptno=10),\n",
       " Row(empno=7844, ename='TURNER', job='SALESMAN', mgr=7698, hiredate='1984-10-08', sal=1500, comm=None, deptno=30),\n",
       " Row(empno=7499, ename='ALLEN', job='SALESMAN', mgr=7698, hiredate='1981-02-20', sal=1600, comm=300, deptno=30),\n",
       " Row(empno=7782, ename='CLARK', job='MANAGER', mgr=7839, hiredate='1981-09-06', sal=2450, comm=None, deptno=10),\n",
       " Row(empno=7698, ename='BLAKE', job='MANAGER', mgr=7839, hiredate='1981-05-01', sal=2850, comm=None, deptno=30),\n",
       " Row(empno=7566, ename='JONES', job='MANAGER', mgr=7839, hiredate='1981-03-02', sal=2975, comm=None, deptno=20),\n",
       " Row(empno=7788, ename='SCOTT', job='ANALYST', mgr=7566, hiredate='1982-12-08', sal=3000, comm=None, deptno=20),\n",
       " Row(empno=7902, ename='FORD', job='ANALYST', mgr=7566, hiredate='1981-12-13', sal=3000, comm=None, deptno=20),\n",
       " Row(empno=7839, ename='KING', job='PRESIDENT', mgr=None, hiredate='1981-11-17', sal=5000, comm=None, deptno=10)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empResult = hiveCtx.sql(\"SELECT * FROM hiveemp order by sal\")\n",
    "empResult.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD를 가지고 임시뷰 생성 ~> SQL을 사용해서 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.createOrReplaceTempView(\"empview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08|1500|null|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25|1300|null|    10|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkdf = spark.sql(\"select * from empview\")\n",
    "print(type(sparkdf))\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "|empno|ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "| 7566|JONES|  MANAGER|7839|1981-03-02|2975|null|    20|\n",
      "| 7698|BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782|CLARK|  MANAGER|7839|1981-09-06|2450|null|    10|\n",
      "| 7788|SCOTT|  ANALYST|7566|1982-12-08|3000|null|    20|\n",
      "| 7839| KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7902| FORD|  ANALYST|7566|1981-12-13|3000|null|    20|\n",
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from empview where sal > 2000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+\n",
      "|deptno|sum(sal)|max(sal)|\n",
      "+------+--------+--------+\n",
      "|    20|   10875|    3000|\n",
      "|    10|    8750|    5000|\n",
      "|    30|    9400|    2850|\n",
      "+------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select deptno, sum(sal), max(sal) from empview group by deptno\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "|empno|ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "| 7566|JONES|  MANAGER|7839|1981-03-02|2975|null|    20|\n",
      "| 7698|BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782|CLARK|  MANAGER|7839|1981-09-06|2450|null|    10|\n",
      "| 7788|SCOTT|  ANALYST|7566|1982-12-08|3000|null|    20|\n",
      "| 7839| KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7902| FORD|  ANALYST|7566|1981-12-13|3000|null|    20|\n",
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from empview where sal > 2000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08|3000|null|    20|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13|3000|null|    20|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02|2975|null|    20|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06|2450|null|    10|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08|1500|null|    30|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25|1300|null|    10|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22|1250|1400|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03|1250| 500|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|null|    30|\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|null|    20|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from empview order by sal desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(empno=7839, ename='KING', job='PRESIDENT', mgr=None, hiredate='1981-11-17', sal=5000, comm=None, deptno=10)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from empview order by sal desc\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KING'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from empview order by sal desc\").take(1)[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![이미지](images/spark_df.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,40\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "row=Row(\"James\",40)\n",
    "print(row[0] +\",\"+str(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\n"
     ]
    }
   ],
   "source": [
    "row=Row(name=\"Alice\", age=11)\n",
    "print(row.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,Alice\n"
     ]
    }
   ],
   "source": [
    "Person = Row(\"name\", \"age\")\n",
    "p1=Person(\"James\", 40)\n",
    "p2=Person(\"Alice\", 35)\n",
    "print(p1.name +\",\"+p2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [Row(name=\"James,,Smith\",lang=[\"Java\",\"Scala\",\"C++\"],state=\"CA\"), \n",
    "    Row(name=\"Michael,Rose,\",lang=[\"Spark\",\"Java\",\"C++\"],state=\"NJ\"),\n",
    "    Row(name=\"Robert,,Williams\",lang=[\"CSharp\",\"VB\"],state=\"NV\")]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,,Smith,['Java', 'Scala', 'C++']\n",
      "Michael,Rose,,['Spark', 'Java', 'C++']\n",
      "Robert,,Williams,['CSharp', 'VB']\n"
     ]
    }
   ],
   "source": [
    "collData=rdd.collect()\n",
    "for row in collData:\n",
    "    print(row.name + \",\" +str(row.lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 날짜데이터를 처리하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|     dates|sum|\n",
      "+----------+---+\n",
      "|2019-05-22|342|\n",
      "|2020-06-02|334|\n",
      "|2019-09-30|269|\n",
      "|2020-10-10|342|\n",
      "|2020-12-25|342|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l1 = [('2019-05-22',342),('2020-06-02',334),('2019-09-30',269),('2020-10-10',342),('2020-12-25',342)]\n",
    "dfl1 =  spark.createDataFrame(l1).toDF(\"dates\",\"sum\")\n",
    "dfl1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+-----+----------+\n",
      "|     dates|sum|years|month|dayofmonth|\n",
      "+----------+---+-----+-----+----------+\n",
      "|2019-05-22|342| 2019|    5|        22|\n",
      "|2020-06-02|334| 2020|    6|         2|\n",
      "|2019-09-30|269| 2019|    9|        30|\n",
      "|2020-10-10|342| 2020|   10|        10|\n",
      "|2020-12-25|342| 2020|   12|        25|\n",
      "+----------+---+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dfl2 = dfl1.withColumn('years',f.year(f.to_timestamp('dates', 'yyyy-MM-dd')))\n",
    "dfl2 = dfl2.withColumn(\"month\",f.month(f.to_timestamp('dates', 'yyyy-MM-dd')))\n",
    "dfl2 = dfl2.withColumn(\"dayofmonth\",f.dayofmonth(f.to_timestamp('dates', 'yyyy-MM-dd')))\n",
    "dfl2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+-----+----------+\n",
      "|     dates|sum|years|month|dayofmonth|\n",
      "+----------+---+-----+-----+----------+\n",
      "|2019-05-22|342| 2019|    5|        22|\n",
      "|2020-06-02|334| 2020|    6|         2|\n",
      "|2019-09-30|269| 2019|    9|        30|\n",
      "|2020-10-10|342| 2020|   10|        10|\n",
      "|2020-12-25|342| 2020|   12|        25|\n",
      "+----------+---+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfl2 = dfl1.withColumn('years',f.year(f.to_timestamp('dates')))\n",
    "dfl2 = dfl2.withColumn(\"month\",f.month(f.to_timestamp('dates')))\n",
    "dfl2 = dfl2.withColumn(\"dayofmonth\",f.dayofmonth(f.to_timestamp('dates')))\n",
    "dfl2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|years|sum(sum)|\n",
      "+-----+--------+\n",
      "| 2019|     611|\n",
      "| 2020|    1018|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfl2.groupBy('years').sum('sum').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NoneType 필터링\n",
    "### pyspark에서 drop method는 NULL을 가진 행을 제거하는데 가장 간단한 함수다. \n",
    "\n",
    "### [drop 메소드에 인수]\n",
    "### any: 모든 행의 컬럼값 중 하나라도 NULL의 값을 가지면 해당 행을 제거\n",
    "### all: 모든 컬럼 값이 NULL이거나 NaN인 경우에만 해당 행을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|  ID|TYPE|CODE|\n",
      "+----+----+----+\n",
      "|   1|   A|  X1|\n",
      "|   2|null|  X2|\n",
      "|   2|   B|  X2|\n",
      "|   2|    |  X1|\n",
      "|null|    |  X3|\n",
      "|   1|   C|  X1|\n",
      "|   2|null|  X1|\n",
      "|   2|   D|null|\n",
      "|null|null|null|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1,'A','X1'),(2,None,'X2'),(2,'B','X2'),(2,'','X1'),(None,'','X3'),(1,'C','X1'),(2,None,'X1'),(2,'D',None),(None,None,None)\n",
    "], [\"ID\", \"TYPE\", \"CODE\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| ID|TYPE|CODE|\n",
      "+---+----+----+\n",
      "|  1|   A|  X1|\n",
      "|  2|   B|  X2|\n",
      "|  2|    |  X1|\n",
      "|  1|   C|  X1|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop('any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|  ID|TYPE|CODE|\n",
      "+----+----+----+\n",
      "|   1|   A|  X1|\n",
      "|   2|null|  X2|\n",
      "|   2|   B|  X2|\n",
      "|   2|    |  X1|\n",
      "|null|    |  X3|\n",
      "|   1|   C|  X1|\n",
      "|   2|null|  X1|\n",
      "|   2|   D|null|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop('all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|  ID|TYPE|CODE|\n",
      "+----+----+----+\n",
      "|   1|   A|  X1|\n",
      "|   2|null|  X2|\n",
      "|   2|   B|  X2|\n",
      "|   2|    |  X1|\n",
      "|null|    |  X3|\n",
      "|   1|   C|  X1|\n",
      "|   2|null|  X1|\n",
      "|   2|   D|null|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop('all', subset=['TYPE', 'CODE']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|  ID|TYPE|CODE|\n",
      "+----+----+----+\n",
      "|   1|   A|  X1|\n",
      "|   2|   B|  X2|\n",
      "|   2|    |  X1|\n",
      "|null|    |  X3|\n",
      "|   1|   C|  X1|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop('any', subset=['TYPE', 'CODE']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|  ID|TYPE|CODE|\n",
      "+----+----+----+\n",
      "|   1|   A|  X1|\n",
      "|   2|null|  X2|\n",
      "|   2|   B|  X2|\n",
      "|   2|    |  X1|\n",
      "|null|    |  X3|\n",
      "|   1|   C|  X1|\n",
      "|   2|null|  X1|\n",
      "|   2|   D|null|\n",
      "|null|null|null|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DK\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------------------+\n",
      "|  Category| ID|               Value|\n",
      "+----------+---+--------------------+\n",
      "|Category A|  1|12.40000000000000...|\n",
      "|Category B|  2|30.10000000000000...|\n",
      "|Category C|  3|                null|\n",
      "|Category D|  4|1.000000000000000000|\n",
      "+----------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "data = [{\"Category\": 'Category A', \"ID\": 1, \"Value\": Decimal(12.40)},\n",
    "        {\"Category\": 'Category B', \"ID\": 2, \"Value\": Decimal(30.10)},\n",
    "        {\"Category\": 'Category C', \"ID\": 3, \"Value\": None},\n",
    "        {\"Category\": 'Category D', \"ID\": 4, \"Value\": Decimal(1.0)},\n",
    "        ]\n",
    "\n",
    "# Create data frame\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------------------+\n",
      "|  Category| ID|               Value|\n",
      "+----------+---+--------------------+\n",
      "|Category A|  1|12.40000000000000...|\n",
      "|Category B|  2|30.10000000000000...|\n",
      "|Category C|  3|                null|\n",
      "|Category D|  4|1.000000000000000000|\n",
      "+----------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "data = [Row(Category='Category A', ID=1, Value= Decimal(12.40)),\n",
    "        Row(Category='Category B', ID=2, Value= Decimal(30.10)),\n",
    "        Row(Category='Category C', ID=3, Value= None),\n",
    "        Row(Category='Category D', ID=4, Value= Decimal(1.0)),\n",
    "        ]\n",
    "\n",
    "# Create data frame\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------------------+\n",
      "|  Category| ID|               Value|\n",
      "+----------+---+--------------------+\n",
      "|Category A|  1|12.40000000000000...|\n",
      "|Category B|  2|30.10000000000000...|\n",
      "|Category D|  4|1.000000000000000000|\n",
      "+----------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Value is not null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+\n",
      "|  Category| ID|Value|\n",
      "+----------+---+-----+\n",
      "|Category C|  3| null|\n",
      "+----------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"Value is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+\n",
      "|  Category| ID|Value|\n",
      "+----------+---+-----+\n",
      "|Category C|  3| null|\n",
      "+----------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Value'].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------------------+\n",
      "|  Category| ID|               Value|\n",
      "+----------+---+--------------------+\n",
      "|Category A|  1|12.40000000000000...|\n",
      "|Category B|  2|30.10000000000000...|\n",
      "|Category D|  4|1.000000000000000000|\n",
      "+----------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.Value.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 날짜타입 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = spark.read.csv(\"data/emp.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['empno', 'ename', 'job', 'mgr', 'hiredate', 'sal', 'comm', 'deptno']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('empno', 'int'),\n",
       " ('ename', 'string'),\n",
       " ('job', 'string'),\n",
       " ('mgr', 'int'),\n",
       " ('hiredate', 'string'),\n",
       " ('sal', 'int'),\n",
       " ('comm', 'int'),\n",
       " ('deptno', 'int')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: integer (nullable = true)\n",
      " |-- hiredate: date (nullable = true)\n",
      " |-- sal: integer (nullable = true)\n",
      " |-- comm: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "newemp = emp.withColumn(\"hiredate\",col(\"hiredate\").cast(\"Date\"))\n",
    "newemp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|year(hiredate)|\n",
      "+--------------+\n",
      "|          1980|\n",
      "|          1981|\n",
      "|          1981|\n",
      "|          1981|\n",
      "|          1981|\n",
      "|          1981|\n",
      "|          1981|\n",
      "|          1982|\n",
      "|          1981|\n",
      "|          1984|\n",
      "|          1983|\n",
      "|          1981|\n",
      "|          1981|\n",
      "|          1982|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp.select(f.year(newemp[\"hiredate\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|month(hiredate)|\n",
      "+---------------+\n",
      "|             12|\n",
      "|              2|\n",
      "|              2|\n",
      "|              3|\n",
      "|             10|\n",
      "|              5|\n",
      "|              9|\n",
      "|             12|\n",
      "|             11|\n",
      "|             10|\n",
      "|              1|\n",
      "|             12|\n",
      "|             12|\n",
      "|              1|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp.select(f.month(newemp[\"hiredate\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|dayofmonth(hiredate)|\n",
      "+--------------------+\n",
      "|                  17|\n",
      "|                  20|\n",
      "|                   3|\n",
      "|                   2|\n",
      "|                  22|\n",
      "|                   1|\n",
      "|                   6|\n",
      "|                   8|\n",
      "|                  17|\n",
      "|                   8|\n",
      "|                  12|\n",
      "|                   3|\n",
      "|                  13|\n",
      "|                  25|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp.select(f.dayofmonth(newemp[\"hiredate\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임시뷰를 활용한 SQL 데이터 처리 복습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.createOrReplaceTempView(\"empview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08|1500|null|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25|1300|null|    10|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkdf = spark.sql(\"select * from empview\")\n",
    "print(type(sparkdf))\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "|empno|ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "| 7566|JONES|  MANAGER|7839|1981-03-02|2975|null|    20|\n",
      "| 7698|BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782|CLARK|  MANAGER|7839|1981-09-06|2450|null|    10|\n",
      "| 7788|SCOTT|  ANALYST|7566|1982-12-08|3000|null|    20|\n",
      "| 7839| KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7902| FORD|  ANALYST|7566|1981-12-13|3000|null|    20|\n",
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from empview where sal > 2000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+\n",
      "|deptno|sum(sal)|max(sal)|\n",
      "+------+--------+--------+\n",
      "|    20|   10875|    3000|\n",
      "|    10|    8750|    5000|\n",
      "|    30|    9400|    2850|\n",
      "+------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select deptno, sum(sal), max(sal) from empview group by deptno\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "|empno|ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "| 7566|JONES|  MANAGER|7839|1981-03-02|2975|null|    20|\n",
      "| 7698|BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782|CLARK|  MANAGER|7839|1981-09-06|2450|null|    10|\n",
      "| 7788|SCOTT|  ANALYST|7566|1982-12-08|3000|null|    20|\n",
      "| 7839| KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7902| FORD|  ANALYST|7566|1981-12-13|3000|null|    20|\n",
      "+-----+-----+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from empview where sal > 2000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08|3000|null|    20|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13|3000|null|    20|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02|2975|null|    20|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06|2450|null|    10|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08|1500|null|    30|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25|1300|null|    10|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22|1250|1400|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03|1250| 500|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|null|    30|\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|null|    20|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from empview order by sal desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(empno=7839, ename='KING', job='PRESIDENT', mgr=None, hiredate='1981-11-17', sal=5000, comm=None, deptno=10)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from empview order by sal desc\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KING'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from empview order by sal desc\").take(1)[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 날짜타입 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .csv(\"data/flight-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|   DEST_COUNTRY_NAME|count(1)|\n",
      "+--------------------+--------+\n",
      "|            Anguilla|       1|\n",
      "|              Russia|       1|\n",
      "|            Paraguay|       1|\n",
      "|             Senegal|       1|\n",
      "|              Sweden|       1|\n",
      "|            Kiribati|       1|\n",
      "|              Guyana|       1|\n",
      "|         Philippines|       1|\n",
      "|            Djibouti|       1|\n",
      "|            Malaysia|       1|\n",
      "|           Singapore|       1|\n",
      "|                Fiji|       1|\n",
      "|              Turkey|       1|\n",
      "|                Iraq|       1|\n",
      "|             Germany|       1|\n",
      "|              Jordan|       1|\n",
      "|               Palau|       1|\n",
      "|Turks and Caicos ...|       1|\n",
      "|              France|       1|\n",
      "|              Greece|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "sqlWay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|count|\n",
      "+--------------------+-----+\n",
      "|            Anguilla|    1|\n",
      "|              Russia|    1|\n",
      "|            Paraguay|    1|\n",
      "|             Senegal|    1|\n",
      "|              Sweden|    1|\n",
      "|            Kiribati|    1|\n",
      "|              Guyana|    1|\n",
      "|         Philippines|    1|\n",
      "|            Djibouti|    1|\n",
      "|            Malaysia|    1|\n",
      "|           Singapore|    1|\n",
      "|                Fiji|    1|\n",
      "|              Turkey|    1|\n",
      "|                Iraq|    1|\n",
      "|             Germany|    1|\n",
      "|              Jordan|    1|\n",
      "|               Palau|    1|\n",
      "|Turks and Caicos ...|    1|\n",
      "|              France|    1|\n",
      "|              Greece|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrameWay = flightData2015\\\n",
    "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "  .count()\n",
    "dataFrameWay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015\\\n",
    "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "  .sum(\"count\")\\\n",
    "  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "  .sort(desc(\"destination_total\"))\\\n",
    "  .limit(5)\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다중 파일도 한방에 읽을 수 있지요..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"data/retail-data/by-day/*.csv\")\n",
    "\n",
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,StringType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,DoubleType,true),StructField(Country,StringType,true)))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staticSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staticDataFrame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from retail_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from retail_data where InvoiceDate > ''\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 윈도우함수(랭킹함수) 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|James        |Sales     |3000  |1         |\n",
      "|Robert       |Sales     |4100  |2         |\n",
      "|Saif         |Sales     |4100  |3         |\n",
      "|Michael      |Sales     |4600  |4         |\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   2|\n",
      "|         Saif|     Sales|  4100|   2|\n",
      "|      Michael|     Sales|  4600|   4|\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 웹사이트에서 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "spark.sparkContext.addFile(\"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv\")\n",
    "df = spark.read.csv(SparkFiles.get(\"adult_data.csv\"), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "|x  |age|workclass|fnlwgt|education   |educational-num|marital-status    |occupation       |relationship|race |gender|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "|1  |25 |Private  |226802|11th        |7              |Never-married     |Machine-op-inspct|Own-child   |Black|Male  |0           |0           |40            |United-States |<=50K |\n",
      "|2  |38 |Private  |89814 |HS-grad     |9              |Married-civ-spouse|Farming-fishing  |Husband     |White|Male  |0           |0           |50            |United-States |<=50K |\n",
      "|3  |28 |Local-gov|336951|Assoc-acdm  |12             |Married-civ-spouse|Protective-serv  |Husband     |White|Male  |0           |0           |40            |United-States |>50K  |\n",
      "|4  |44 |Private  |160323|Some-college|10             |Married-civ-spouse|Machine-op-inspct|Husband     |Black|Male  |7688        |0           |40            |United-States |>50K  |\n",
      "|5  |18 |?        |103497|Some-college|10             |Never-married     |?                |Own-child   |White|Female|0           |0           |30            |United-States |<=50K |\n",
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|fnlwgt|\n",
      "+---+------+\n",
      "| 25|226802|\n",
      "| 38| 89814|\n",
      "| 28|336951|\n",
      "| 44|160323|\n",
      "| 18|103497|\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age','fnlwgt').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|   education|count|\n",
      "+------------+-----+\n",
      "|   Preschool|   83|\n",
      "|     1st-4th|  247|\n",
      "|     5th-6th|  509|\n",
      "|   Doctorate|  594|\n",
      "|        12th|  657|\n",
      "|         9th|  756|\n",
      "| Prof-school|  834|\n",
      "|     7th-8th|  955|\n",
      "|        10th| 1389|\n",
      "|  Assoc-acdm| 1601|\n",
      "|        11th| 1812|\n",
      "|   Assoc-voc| 2061|\n",
      "|     Masters| 2657|\n",
      "|   Bachelors| 8025|\n",
      "|Some-college|10878|\n",
      "|     HS-grad|15784|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"education\").count().sort(\"count\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n",
      "|summary|                 x|               age|  workclass|            fnlwgt|   education|   educational-num|marital-status|      occupation|relationship|              race|gender|      capital-gain|     capital-loss|    hours-per-week|native-country|income|\n",
      "+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n",
      "|  count|             48842|             48842|      48842|             48842|       48842|             48842|         48842|           48842|       48842|             48842| 48842|             48842|            48842|             48842|         48842| 48842|\n",
      "|   mean|           24421.5| 38.64358543876172|       null|189664.13459727284|        null|10.078088530363212|          null|            null|        null|              null|  null|1079.0676262233324|87.50231358257237|40.422382375824085|          null|  null|\n",
      "| stddev|14099.615260708357|13.710509934443502|       null|105604.02542315757|        null| 2.570972755592252|          null|            null|        null|              null|  null| 7452.019057655413|403.0045521243591|12.391444024252289|          null|  null|\n",
      "|    min|                 1|                17|          ?|             12285|        10th|                 1|      Divorced|               ?|     Husband|Amer-Indian-Eskimo|Female|                 0|                0|                 1|             ?| <=50K|\n",
      "|    max|             48842|                90|Without-pay|           1490400|Some-college|                16|       Widowed|Transport-moving|        Wife|             White|  Male|             99999|             4356|                99|    Yugoslavia|  >50K|\n",
      "+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      capital-gain|\n",
      "+-------+------------------+\n",
      "|  count|             48842|\n",
      "|   mean|1079.0676262233324|\n",
      "| stddev| 7452.019057655413|\n",
      "|    min|                 0|\n",
      "|    max|             99999|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('capital-gain').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20211"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.age > 40).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다양한 집계(aggregation) 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n",
      "approx_count_distinct: 6\n",
      "avg: 3444.4444444444443\n",
      "+------------------------------------------------------+\n",
      "|collect_list(salary)                                  |\n",
      "+------------------------------------------------------+\n",
      "|[4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
      "+------------------------------------------------------+\n",
      "\n",
      "+------------------------------------+\n",
      "|collect_set(salary)                 |\n",
      "+------------------------------------+\n",
      "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
      "+------------------------------------+\n",
      "\n",
      "+----------------------------------+\n",
      "|count(DISTINCT department, salary)|\n",
      "+----------------------------------+\n",
      "|8                                 |\n",
      "+----------------------------------+\n",
      "\n",
      "Distinct Count of Department & Salary: 8\n",
      "count: Row(count(salary)=9)\n",
      "+-------------+\n",
      "|first(salary)|\n",
      "+-------------+\n",
      "|4600         |\n",
      "+-------------+\n",
      "\n",
      "+------------+\n",
      "|last(salary)|\n",
      "+------------+\n",
      "|4100        |\n",
      "+------------+\n",
      "\n",
      "+-------------------+\n",
      "|kurtosis(salary)   |\n",
      "+-------------------+\n",
      "|-0.6953888522988017|\n",
      "+-------------------+\n",
      "\n",
      "+-----------+\n",
      "|max(salary)|\n",
      "+-----------+\n",
      "|4600       |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|min(salary)|\n",
      "+-----------+\n",
      "|2000       |\n",
      "+-----------+\n",
      "\n",
      "+------------------+\n",
      "|avg(salary)       |\n",
      "+------------------+\n",
      "|3444.4444444444443|\n",
      "+------------------+\n",
      "\n",
      "+--------------------+\n",
      "|skewness(salary)    |\n",
      "+--------------------+\n",
      "|-0.28089138971711725|\n",
      "+--------------------+\n",
      "\n",
      "+-------------------+-------------------+------------------+\n",
      "|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n",
      "+-------------------+-------------------+------------------+\n",
      "|798.6099033807293  |798.6099033807293  |752.9366376043296 |\n",
      "+-------------------+-------------------+------------------+\n",
      "\n",
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|31000      |\n",
      "+-----------+\n",
      "\n",
      "+--------------------+\n",
      "|sum(DISTINCT salary)|\n",
      "+--------------------+\n",
      "|20900               |\n",
      "+--------------------+\n",
      "\n",
      "+-----------------+-----------------+-----------------+\n",
      "|var_samp(salary) |var_samp(salary) |var_pop(salary)  |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|637777.7777777778|637777.7777777778|566913.5802469136|\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
    "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
    "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
    "from pyspark.sql.functions import variance,var_samp,  var_pop\n",
    "\n",
    "simpleData = [\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "  \n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "print(\"approx_count_distinct: \" + \\\n",
    "      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n",
    "\n",
    "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n",
    "\n",
    "df.select(collect_list(\"salary\")).show(truncate=False)\n",
    "\n",
    "df.select(collect_set(\"salary\")).show(truncate=False)\n",
    "\n",
    "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
    "df2.show(truncate=False)\n",
    "print(\"Distinct Count of Department & Salary: \"+str(df2.collect()[0][0]))\n",
    "\n",
    "print(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\n",
    "df.select(first(\"salary\")).show(truncate=False)\n",
    "df.select(last(\"salary\")).show(truncate=False)\n",
    "df.select(kurtosis(\"salary\")).show(truncate=False)\n",
    "df.select(max(\"salary\")).show(truncate=False)\n",
    "df.select(min(\"salary\")).show(truncate=False)\n",
    "df.select(mean(\"salary\")).show(truncate=False)\n",
    "df.select(skewness(\"salary\")).show(truncate=False)\n",
    "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n",
    "    stddev_pop(\"salary\")).show(truncate=False)\n",
    "df.select(sum(\"salary\")).show(truncate=False)\n",
    "df.select(sumDistinct(\"salary\")).show(truncate=False)\n",
    "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF(User Defined Function) 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[empno: int, ename: string, job: string, mgr: int, hiredate: string, sal: int, comm: int, deptno: int]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detQuarter(sal):\n",
    "    Q = 'E'\n",
    "    if(sal > 4000):\n",
    "        Q = 'A'\n",
    "    elif(sal > 3000):\n",
    "        Q = 'B'\n",
    "    elif(sal > 2000):\n",
    "        Q = 'C'\n",
    "    elif(sal > 1000):\n",
    "        Q = 'D'\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "grade = udf(detQuarter, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+-----+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|grade|\n",
      "+-----+------+---------+----+----------+----+----+------+-----+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|null|    20|    E|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|    D|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03|1250| 500|    30|    D|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02|2975|null|    20|    C|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22|1250|1400|    30|    D|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|    C|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06|2450|null|    10|    C|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08|3000|null|    20|    C|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|    A|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08|1500|null|    30|    D|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12|1100|null|    20|    D|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|null|    30|    E|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13|3000|null|    20|    C|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25|1300|null|    10|    D|\n",
      "+-----+------+---------+----+----------+----+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp = emp.withColumn(\"grade\", grade('sal'))\n",
    "newemp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convertCase(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    return resStr \n",
    "\n",
    "convertUDF = udf(lambda z: convertCase(z))\n",
    "\n",
    "df.select(col(\"Seqno\"), \\\n",
    "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
    ".show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydatavenv",
   "language": "python",
   "name": "pydatavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
